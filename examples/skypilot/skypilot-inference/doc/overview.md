# Serving LLMs with vLLM: A Practical Inference Guide

This guide teaches the essentials of serving large language models (LLMs) with vLLM. It explains inference basics, model artifacts, vLLM’s core ideas, how vLLM executes requests, and how to prepare and configure models for production inference.

---

## 1) What is Inference?

Inference is using a trained model to produce outputs for new inputs. For LLMs, that means generating text (completions, answers, or chat replies) from a prompt or conversation history.

- Input: prompt text (completions) or role-structured messages (chat)
- Output: generated tokens streamed or returned as a full response
- Two important phases at runtime are as follows:
  - Prefill: process the whole prompt/history to initialize state
  - Decode: generate tokens step-by-step using cached attention state

Key benchmarking metrics for LLM inference:

- **Latency**: Measures how quickly the model responds. Two common units:
  - *Time to first output token*: How long it takes from sending your prompt to receiving the first generated token (the start of the model’s reply).
  - *Time to full response*: How long it takes to receive the entire generated output (all output tokens).
  - Lower latency means faster, more interactive responses.

- **Throughput**: Measures how much work the system can handle. Common units:
  - *Requests per second (RPS)*: How many separate prompts or queries the system can process per second.
  - *Tokens per second (TPS)*: How many output tokens the model can generate per second (across all requests).
  - Higher throughput means more users or longer outputs can be served efficiently.

- **Concurrency**: Number of requests or users served at the same time. Important for real-world deployments.

- **Memory usage**: Amount of GPU/CPU memory consumed during inference. Affects how many requests or how large a context you can serve.

These metrics help you compare different models, hardware setups, and serving configurations. For most users, latency (especially time to first output token) and throughput (tokens per second) are the most important for user experience and scaling.


### How LLM Inference Works: Step-by-Step Workflow

To help you understand the process, here’s a simple workflow for how large language model (LLM) inference works.

1. **Input Preparation**: You provide a prompt (text or chat history) to the model.
2. **Tokenization**: The model converts your text into input tokens (numbers representing words or pieces of words). These are the tokens that represent your question or prompt.
3. **Prefill Phase**: The model processes all input tokens to set up its internal state (memory for context).
4. **Decoding Phase**: The model generates output tokens one by one, these are the new tokens that form its answer or completion.
5. **Sampling**: At each step, the model considers many possible next output tokens, each with a probability score. It does not generate all of them—instead, it uses the sampling parameters (like temperature, top_k, top_p) to pick one token from the most likely candidates. This is what makes the output creative or focused, depending on your settings. (See below for details.)
6. **Detokenization**: The output tokens are converted back into readable text.
7. **Output**: The final text (completion or chat reply) is returned to you.

Some settings (like sampling parameters) affect how creative or focused the output is. The workflow repeats steps 4–6 until the desired number of tokens is generated or a stop condition is met.
Note: “input tokens” are the tokens created from your prompt; “output tokens” are the new tokens generated by the model as its response.

---

### Terminology and building blocks:
- Tokens and tokenizers: deterministic mapping between text and token IDs
- Context window: (maximum tokens per prompt; e.g., if a model has a context window of 2,000 tokens, it can process up to 2,000 tokens in your prompt or conversation history.
- Sampling: temperature, top_p/top_k, max_tokens, penalties, stop sequences

**Sampling parameters**

- **temperature**: Controls randomness. Low temperature (e.g., 0.1) makes the model more predictable and focused; high temperature (e.g., 1.0) makes it more creative and varied. Used in the decoding phase.
- **top_p**: Also called nucleus sampling. The 'p' stands for probability. The model considers only the smallest set of tokens whose combined probability is at least p (e.g., 0.9), then picks from them. For example, if top_p=0.8, the model will only consider tokens whose combined probability is at least 80%.
- **top_k**: The 'k' stands for the number of tokens. The model considers only the top k most likely tokens (e.g., top 50) and picks from them. For example, if top_k=5, the model will only consider the 5 most likely tokens at each step.
- **max_tokens**: The maximum number of tokens the model will generate in its output. Used to limit the length of the response. Set before decoding starts.
- **penalties**: These adjust the likelihood of repeating words or phrases. For example, a repetition penalty discourages the model from repeating itself. Applied during decoding.
- **stop sequences**: Specific words or phrases that, if generated, will cause the model to stop generating further output. Used to end the response early if needed. Checked during decoding.

All these settings are used in the **decoding phase** of inference, which is when the model is generating new tokens one by one.
- Streaming: send tokens as they are generated for interactivity
- KV cache: reuse past attention states for O(1) per-token decode cost
- Batching: group requests to amortize compute and maximize GPU utilization

KV cache, paging, and batching at a glance:
- Without caching, each new token would re-attend the full prompt/history
- vLLM implements a paged KV cache (PagedAttention) that:
  - Allocates fixed-size memory blocks on demand per sequence
  - Shares prefix blocks across requests (prefix caching)
  - Enables continuous batching and fair preemption

Costs and limits:
- Memory ~ concurrent sequences × effective context × hidden size × dtype
- Larger batches and context improve throughput but may raise latency and memory use

### What is a Transformer?
- A Transformer is a neural network architecture built around self‑attention and feed‑forward layers, arranged in repeated blocks. It does not “transform text to numbers” directly—that’s tokenization. Instead, text is tokenized to IDs, embedded as vectors, then processed by stacked attention + MLP blocks to model dependencies across the sequence.
- Why it matters for inference: Transformers enable strong long‑range context handling via attention. Prefill cost scales roughly O(L²) with prompt length L, while decode is O(1) per new token with a KV cache. Serving Transformers benefits from specialized attention kernels, KV paging, and continuous batching (vLLM’s strengths). Non‑Transformer architectures (e.g., classic RNNs) don’t use attention the same way and have different scaling/serving characteristics.

### Context defined
- Context (or context window) is the span of tokens the model can consider at once. It includes your prompt, system instructions, chat history, and any assistant output fed back for continuity. If total tokens exceed the model’s maximum context length, the earliest tokens must be truncated or summarized. Larger context supports richer tasks (RAG, long chats) but consumes more memory and increases prefill latency.

### Vocabulary (vocab) defined
- “Vocab” is the set of tokens the tokenizer can emit. Larger vocabularies (e.g., ~150k in Qwen) can encode some languages/scripts more efficiently, potentially reducing token counts for the same text. Different tokenizers (BPE, SentencePiece, tiktoken‑derived) segment text differently; this affects token counts, latency, and cost. Always use the tokenizer intended for the model and be cautious when switching variants.

### Attention defined

**Attention explained (human analogy):**
Imagine reading a book and trying to answer a question about the story. Your brain doesn’t just focus on the last sentence—you recall relevant details from earlier pages, weighing which memories matter most for your answer. In AI, attention is the mechanism that lets a model do something similar: for each new word it generates, it looks back at all previous words (context), deciding which parts are most important for the next step. This is not true memorization, but dynamic focus—like how you might remember a plot twist or a character’s name when needed, but not every word you’ve read.

**Does attention mean the AI memorizes context?**
Not exactly. Attention lets the model dynamically focus on relevant parts of the context window (prompt, history, instructions) for each output token. It does not store or recall information like human memory, but it can “integrate” previous context by weighting and combining it at each step. The KV cache is a technical optimization that lets the model reuse these computed weights efficiently, so it can generate long outputs without reprocessing the entire prompt every time.

**Attention, at a glance**
- What it is and why it matters: Attention lets a token “look back” at the prompt and prior tokens to decide what to generate next. In Transformer LLMs, self‑attention computes, for each token, weighted combinations of all earlier token representations. This enables long‑range dependencies and contextual reasoning that n‑gram or fixed‑window models cannot capture.
- Positional encodings: models bake this in during training and it constrains backend choice.
  - ALiBi: additive linear bias by distance; used by BLOOM-176B.
  - RoPE: rotary embeddings; common in LLaMA/Qwen; sometimes extended for long context.
- Important: you cannot change a model’s attention mechanism at serve time; you only choose the implementation kernel (backend) compatible with it. See “Attention backends: how to choose”.

---

## 3) Models: Architecture and Artifacts

When you download a model (e.g., from Hugging Face), you get more than weights:
- Weights (safetensors shards + index): the learned parameters (dominant size)
- Config (config.json): architecture hyperparameters and positional strategy
- Tokenizer assets: tokenizer.json, tokenizer_config.json, vocab/merges, specials
- Generation defaults (optional): generation_config.json
- Adapters (optional): LoRA/PEFT weights for fine-tuned variants
- Custom code (rare): trust_remote_code

Tokenizer pipeline (why it matters):
1) Normalize text → 2) Pre-tokenize → 3) Subword segmentation (BPE/SP/WordPiece)
4) Post-process (BOS/EOS) → 5) Map to IDs; reverse for detokenization

Minimal example (Hugging Face Transformers):
```python
from transformers import AutoTokenizer
model_id = "bigscience/bloom"
tok = AutoTokenizer.from_pretrained(model_id)
ids = tok.encode("Write a short poem about the moon.", add_special_tokens=True)
print(ids)
print(tok.decode(ids))
```

Operational implications:
- Token counts drive latency and cost; tokenizers differ across models
- Ensure tokenizer and weights are from the same repo/revision

### Licenses: what to check
- License type: fully open source, research‑only, or restricted/commercial. Examples here: BLOOM RAIL (open with use constraints), Tongyi Qianwen license (commercial allowed with terms).
- Commercial use: verify if allowed and under what conditions; some require registration or approval for commercial deployments.
- Redistribution and derivatives: check whether you can redistribute weights, fine‑tuned variants, or quantized artifacts.
- Attribution and restrictions: some licenses include RAIL‑style acceptable‑use clauses or attribution requirements.
- Practical guidance: read the model card’s license section and linked license text; follow any registration steps if required by the provider. When in doubt, consult your legal/compliance team.

### Model profiles and selection: BLOOM-176B vs Qwen-72B
Use official model cards for authoritative specs; below are practitioner notes with links.

- BLOOM-176B (bigscience/bloom)
  - Size and memory: 176B parameters; BF16/FP16 weights alone are ~352 GB. Expect multi-node tensor parallelism and/or quantization for serving.
  - Context and positions: trained with ~2k context and ALiBi positional bias. ALiBi impacts backend choice (avoid FA3; prefer Torch SDPA or Triton).
  - Tokenizer and prompts: HF fast tokenizer; no built-in chat template—provide one for chat-style prompts.
  - Languages: multilingual; check card for coverage. License: BigScience BLOOM RAIL 1.0.
  - Card: https://huggingface.co/bigscience/bloom

- Qwen-72B (Qwen/Qwen-72B)
  - Size and memory: 72B parameters. Authors note BF16/FP16 chat requires on the order of ~144 GB total GPU memory (e.g., 2×A100-80G or 5×V100-32G); INT4 variants can fit ≈48 GB. Plan TP accordingly.
  - Context and positions: supports 32k context via extended RoPE; backend kernels like FlashAttention v2/v3 are typically supported; SDPA is a safe fallback.
  - Tokenizer and prompts: tiktoken-derived large vocab (~152k). Some Transformers flows require trust_remote_code; ensure your runtime matches the model version. Chat variants may provide templates.
  - License: Tongyi Qianwen license; review for commercial use terms.
  - Card: https://huggingface.co/Qwen/Qwen-72B (newer: Qwen1.5-72B)

Choosing between them for an inference task
- Hardware fit: check total VRAM and interconnect; BLOOM-176B generally needs multi-node TP or heavy quantization. Qwen-72B is easier to deploy on fewer high-memory GPUs or with INT4.
- Context needs: if you require >8k context, Qwen-72B’s 32k support is advantageous. BLOOM typically serves around 2k unless specialized. See context-defined.
- Language/compatibility: ensure tokenizer and chat templates align with your inputs; Qwen’s large vocab helps multilingual inputs; BLOOM is broadly multilingual too.
- Backend compatibility: BLOOM’s ALiBi favors Torch SDPA/Triton; Qwen with RoPE can leverage FlashAttention for best throughput when available.
- License and ecosystem: verify your use case aligns with each model’s license; consider community adapters and quantized checkpoints.

---

## 4) vLLM: Core Concepts and Features

vLLM is an inference engine optimized for high throughput and efficiency:
- PagedAttention (paged KV cache) and prefix caching
- Continuous batching of incoming requests
- Optimized attention backends (FlashAttention/FlashInfer/SDPA/TRITON_ATTN)
- Speculative decoding, chunked/disaggregated prefill
- Quantization support (GPTQ, AWQ, INT4/INT8/FP8; KV quant when available)
- Multi-LoRA, multimodal support
- OpenAI-compatible API server (completions/chat/models)
- Metrics and logging for production ops

Supported hardware: NVIDIA (CUDA), AMD (HIP/ROCm), Intel, PowerPC, TPU; pick images/builds compatible with your accelerator.

### Attention backends: how to choose
You select an implementation kernel compatible with the model’s attention and your hardware:
- Torch SDPA (baseline): robust and widely compatible (ALiBi, RoPE). Use when unsure or if other kernels are unstable.
- FlashAttention v2/v3: fastest on NVIDIA when supported; requires compatible head dims and positional encodings (not ALiBi). Great fit for RoPE models like Qwen.
- Triton attention: good alternative on NVIDIA for ALiBi models when you want more speed than SDPA.
- FlashInfer and other backends: specialized high-performance options depending on build. Verify support matrix for your device.

Decision guide
- If model uses ALiBi (e.g., BLOOM-176B): pick Torch SDPA or Triton; avoid FA3. Confirm in logs the kernel actually selected.
- If model uses RoPE and your build includes FlashAttention: pick flash-attn; fall back to SDPA if unsupported.
- On instability during warmup or capture: force SDPA and eager mode first; introduce faster kernels incrementally.

Quick mapping (positional encoding → backends)
- ALiBi: Torch SDPA (safe), Triton (often faster than SDPA), avoid FA3.
- RoPE (standard dims): FlashAttention v2/v3 (fastest on NVIDIA), else SDPA.
- RoPE (nonstandard head dims/build limits): SDPA fallback.
- Unknown/experimental: start with SDPA; verify logs before switching.

CUDA graphs and compile mode:
- Capturing CUDA graphs reduces launch overhead after warmup
- Some stacks are sensitive; eager mode is the robust baseline
- You can disable graphs (e.g., enforce eager) and re-enable after validation

---

## 5) How vLLM Works (Request Flow and API)

High-level flow:
1) Load tokenizer/assets and weights; initialize tensor-parallel ranks
2) Start OpenAI-compatible server on the configured host/port
3) For each request: tokenize → schedule/batch → prefill/decode → detokenize
4) Stream or return final text; update/reuse KV blocks for subsequent tokens

Completions (classic prompt → continuation):
```json
{
  "model": "bigscience/bloom",
  "prompt": "Complete: The benefits of tensor parallelism are",
  "max_tokens": 64,
  "temperature": 0.7,
  "stream": true
}
```

Chat completions (role-structured messages → assistant reply):
```json
{
  "model": "bigscience/bloom",
  "messages": [
    {"role": "user", "content": "Write a one-line haiku about GPUs."}
  ],
  "max_tokens": 64,
  "temperature": 0.7,
  "stream": true
}
```

Chat templates (Transformers ≥ 4.44):
- If a tokenizer lacks a built-in chat template (e.g., BLOOM), you must provide one
- Use a simple Jinja template that formats messages into a single prompt

Health check and examples:
```
curl -fsS http://<HOST>:8000/v1/models
curl -s http://<HOST>:8000/v1/completions -H 'Content-Type: application/json' -d '{"model":"bigscience/bloom","prompt":"Write a short poem about the moon.","max_tokens":64}'
```

---

## 6) Preparing and Configuring Models for Inference

Make these choices before going live:
- Model and revision: pick weights; ensure tokenizer matches
- Dtype: bfloat16 recommended on H100/H200; fp16 where appropriate
- Attention backend: pick a kernel compatible with positional encoding
- For BLOOM/ALiBi, prefer Torch SDPA or Triton attention; avoid FA3
- Parallelism: set `--tensor-parallel-size` to GPUs per node
- Limits: `--max-model-len`, `--max-num-seqs` to fit memory and target latency
- KV cache: plan memory footprint; consider KV quantization if supported
- Chat template: required for chat if tokenizer has none; pass via `--chat-template`
- Quantization: choose GPTQ/AWQ/INT8/INT4/FP8 variants if compatible
- Eager vs graphs: start with eager; enable CUDA graphs after validation
- Observability: enable metrics and set logging level appropriately

Minimal readiness checklist:
- Weights/tokenizer verified; dtype set (bf16 on Hopper)
- Attention backend confirmed in logs (no FA3 for ALiBi models)
- TP size equals number of local GPUs; health probe returns models
- Max context and concurrency tuned; no OOMs during warmup/tests
- Chat template provided when required; 200 responses for both endpoints

Driver/Runtime compatibility (CUDA):
- Host driver CUDA version must be >= container’s build CUDA version
- Validate with `nvidia-smi` (host) and `torch.version.cuda` (container)

---

## 7) Operational Notes and Troubleshooting

Common issue: BLOOM + ALiBi with FlashAttention v3 (FA3)
- Symptom: first request crashes with `AssertionError: Alibi is not supported in FA3`
- Fix: force Torch SDPA (e.g., `--attention-backend torch-sdpa` or `-O.attention_backend=TORCH_SDPA`); keep eager mode if unstable
- Note: Some builds may still route to FA internally; verify backend in logs

CUDA graphs stability
- If warmup or capture crashes, disable graphs (enforce eager), align driver/toolkit to image, then re-enable progressively

Health checks
- Probe `/v1/models`; only proceed when server is bound and healthy

Performance tuning
- Increase batch/concurrency for throughput; monitor latency and KV memory
- Stream responses to improve perceived latency

Security and production hygiene
- Add TLS, auth, rate limits; expose metrics; set resource limits; avoid anonymous public endpoints

---

Appendix: Quick Reference
- Key flags: `--tensor-parallel-size`, `--dtype`, `--download-dir`, `--max-model-len`, `--max-num-seqs`, `--chat-template`
- Where to look in logs: selected attention backend, CUDA graphs capture, health/ready
- Helpful envs (varies by build): `VLLM_ATTENTION_BACKEND`, `VLLM_DISABLE_CUDA_GRAPHS`, `VLLM_COMPILE_MODE`
