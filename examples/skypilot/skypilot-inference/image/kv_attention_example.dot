// Graphviz diagram: KV cache and attention score calculation for 3 tokens
// Save as: kv_attention_example.dot

digraph KVAttention {
  rankdir=LR;
  node [shape=box, fontname="Arial"];

  // Tokens

  // Color definitions

  // Tokens
  // Force vertical order: how (top), are (middle), you (bottom)
  T0 [label="Input Token 0: how", style=filled, fillcolor="#ffe4b2", pos="0,3!"]
  T1 [label="Input Token 1: are", style=filled, fillcolor="#ffe4b2", pos="0,2!"]
  T2 [label="Input Token 2: you", style=filled, fillcolor="#ffe4b2", pos="0,1!"]

  // Embeddings
  E0 [label="Embedding 0\n[vector]", shape=ellipse, style=filled, fillcolor="#b2e4ff"]
  E1 [label="Embedding 1\n[vector]", shape=ellipse, style=filled, fillcolor="#b2e4ff"]
  E2 [label="Embedding 2\n[vector]", shape=ellipse, style=filled, fillcolor="#b2e4ff"]

  // QKV
  Q0 [label="Query 0\n[vector]", shape=ellipse, style=filled, fillcolor="#ffd1dc"]
  K0 [label="Key 0\n[vector]", shape=ellipse, style=filled, fillcolor="#e6b2ff"]
  V0 [label="Value 0\n[vector]", shape=ellipse, style=filled, fillcolor="#b2ffb2"]

  Q1 [label="Query 1\n[vector]", shape=ellipse, style=filled, fillcolor="#ffd1dc"]
  K1 [label="Key 1\n[vector]", shape=ellipse, style=filled, fillcolor="#e6b2ff"]
  V1 [label="Value 1\n[vector]", shape=ellipse, style=filled, fillcolor="#b2ffb2"]

  Q2 [label="Query 2\n[vector]", shape=ellipse, style=filled, fillcolor="#ffd1dc"]
  K2 [label="Key 2\n[vector]", shape=ellipse, style=filled, fillcolor="#e6b2ff"]
  V2 [label="Value 2\n[vector]", shape=ellipse, style=filled, fillcolor="#b2ffb2"]

  // KV Cache
  subgraph cluster_kvcache {
    label="KV Cache (after 3 tokens)";
    style=dashed;
    K0; V0; K1; V1; K2; V2;
  }

  // Attention score calculation for Token 2
  Q2K0 [label="Dot(Q2, K0)\n= score 0", shape=diamond, style=filled, fillcolor="#e0e0ff"]
  Q2K1 [label="Dot(Q2, K1)\n= score 1", shape=diamond, style=filled, fillcolor="#e0e0ff"]
  Q2K2 [label="Dot(Q2, K2)\n= score 2", shape=diamond, style=filled, fillcolor="#e0e0ff"]

  // Softmax
  SM [label="Softmax\n(scores)", shape=parallelogram, style=filled, fillcolor="#ffe0e0"]

  // Weighted sum
    WS [label="Weighted sum:\nSoftmax(score0) × V0 +\nSoftmax(score1) × V1 +\nSoftmax(score2) × V2\n\nWeightedSum = Σ w_i V_i", shape=note, style=filled, fillcolor="#e0ffe0"]

  // Logits and output token
    LOGITS [label="Project to logits:\nlogits = WeightedSum × W_vocab\n(1 score per vocab token)", shape=box, style=filled, fillcolor="#e0f7fa"]
  ARGMAX [label="Select output token:\noutput_id = argmax(logits)", shape=box, style=filled, fillcolor="#f0e0fa"]
  DETOK [label="Detokenize:\noutput_text = tokenizer.decode(output_id)", shape=box, style=filled, fillcolor="#f0fff0"]

  // Output tokens
  OUT0 [label="Output Token 0: I", style=filled, fillcolor="#c2ffc2"]
  OUT1 [label="Output Token 1: am", style=filled, fillcolor="#c2ffc2"]

  // Connections
  // Explicitly order input token connections for top-down: how, are, you
  T0 -> E0 -> Q0
  E0 -> K0
  E0 -> V0
  T1 -> E1 -> Q1
  E1 -> K1
  E1 -> V1
  T2 -> E2 -> Q2
  E2 -> K2
  E2 -> V2

  // Q2 attention calculation
  Q2 -> Q2K0
  K0 -> Q2K0
  Q2 -> Q2K1
  K1 -> Q2K1
  Q2 -> Q2K2
  K2 -> Q2K2

  Q2K0 -> SM
  Q2K1 -> SM
  Q2K2 -> SM

  SM -> WS
  V0 -> WS
  V1 -> WS
  V2 -> WS
  WS -> LOGITS
  LOGITS -> ARGMAX
  ARGMAX -> DETOK
  DETOK -> OUT0
  OUT0 -> OUT1 [style=dashed, label="next output"]

  // Legend and formulas
  // Place Descriptions and Legend side by side using rank=same and invisible edge
  subgraph cluster_legends {
    label="";
    color=white;
    rank=same;
    descriptions [label=<
      <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0" CELLPADDING="4" ALIGN="LEFT">
        <TR><TD><B>Descriptions</B></TD></TR>
        <TR><TD ALIGN="LEFT">KV cache stores all Keys/Values</TD></TR>
        <TR><TD ALIGN="LEFT">For new token, Query is compared (dot product) with all Keys</TD></TR>
        <TR><TD ALIGN="LEFT">Softmax turns scores into weights</TD></TR>
        <TR><TD ALIGN="LEFT">Output is weighted sum of Values</TD></TR>
          <TR><TD ALIGN="LEFT">Logits: WeightedSum × W_vocab</TD></TR>
        <TR><TD ALIGN="LEFT">Output token: argmax(logits)</TD></TR>
        <TR><TD ALIGN="LEFT">Detokenize: tokenizer.decode(output_id)</TD></TR>
        <TR><TD ALIGN="LEFT">(Repeat for each token)</TD></TR>
      </TABLE>
    >, shape=box, style=dotted, fontsize=10];
    legend [label=<
      <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0" CELLPADDING="4" ALIGN="LEFT">
        <TR><TD><B>Legend</B></TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffe4b2">Input Token</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#c2ffc2">Output Token</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#b2e4ff">Embedding</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffd1dc">Query</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e6b2ff">Key</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#b2ffb2">Value</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e0e0ff">Dot (Attention Score)</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffe0e0">Softmax</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e0f7fa">Logits</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#f0e0fa">Argmax</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#f0fff0">Detokenization</TD></TR>
      </TABLE>
    >, shape=box, style=dotted, fontsize=10];
      descriptions -> legend [style=invis];
    legend [label=<
      <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0" CELLPADDING="4" ALIGN="LEFT">
        <TR><TD><B>Legend</B></TD><TD><B>Formula</B></TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffe4b2">Input Token</TD><TD ALIGN="LEFT"></TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#c2ffc2">Output Token</TD><TD ALIGN="LEFT"></TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#b2e4ff">Embedding</TD><TD ALIGN="LEFT"></TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffd1dc">Query</TD><TD ALIGN="LEFT">Q = xW_Q</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e6b2ff">Key</TD><TD ALIGN="LEFT">K = xW_K</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#b2ffb2">Value</TD><TD ALIGN="LEFT">V = xW_V</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e0e0ff">Dot (Attention Score)</TD><TD ALIGN="LEFT">score_i = Q · K_i</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#ffe0e0">Softmax</TD><TD ALIGN="LEFT">w_i = exp(score_i) / Σ_j exp(score_j)</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e0ffe0">Weighted sum</TD><TD ALIGN="LEFT">Σ_i w_i V_i</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#e0f7fa">Logits</TD><TD ALIGN="LEFT">logits = WeightedSum × W_vocab</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#f0e0fa">Argmax</TD><TD ALIGN="LEFT">output_id = argmax(logits)</TD></TR>
        <TR><TD ALIGN="LEFT" BGCOLOR="#f0fff0">Detokenization</TD><TD ALIGN="LEFT">output_text = tokenizer.decode(output_id)</TD></TR>
      </TABLE>
    >, shape=box, style=dotted, fontsize=10];
  }
}
