Nebius VPN Gateway (VM-based) — Architecture & Design

Version: v0.1 (scaffold-aligned)

Summary
- VM-based Site‑to‑Site VPN gateway for Nebius AI Cloud.
- IPsec (strongSwan) + BGP (FRR) or static routing.
- YAML‑driven desired state with optional peer config import (GCP/AWS/Azure/Cisco).
- Idempotent agent running on each gateway VM; orchestrator CLI applies changes.

1) High‑Level Architecture
Components
- Orchestrator CLI (`nebius-vpngw`): runs on operator laptop/CI. Reads the local YAML and optional peer configs, merges to a resolved plan, ensures VMs, pushes per‑VM configs over SSH, and triggers the agent reload.
- Gateway VM (Nebius Compute): Ubuntu LTS base; packages: strongSwan, FRR, Python runtime, and the always‑running agent.
- Agent (`nebius-vpngw-agent`): reads a per‑VM resolved config, renders strongSwan/FRR configs, applies them idempotently, and records last‑applied state.
- Optional HA Route Controller: updates VPC route next‑hop to switch between gateways when using multi‑VM setups.

Goals
- IKEv2 (default) with optional IKEv1; PSK auth; AES‑256; SHA‑256/384/512; DH 14/20/24; configurable lifetimes.
- BGP preferred; static routing supported per tunnel/connection.
- Works with GCP HA VPN, AWS Site‑to‑Site VPN, Azure VPN Gateway, and on‑prem routers.
- Two deployment modes:
  - Single VM with multiple tunnels (active/active at tunnel level; VM is SPOF).
  - Gateway group (N VMs) with route switching for VM‑level HA.

Current code alignment
- Package layout under `src/nebius_vpngw` matches design (cli, config loader, peer parsers, deploy managers, agent with state store/renderer stubs).
- VM/route/SSH managers are scaffolds; replace with Nebius SDK and SSH implementations to make it fully functional.

2) Nebius Topology & Routing Model
- Single VPC network selected by `network_id` (optional). If omitted, the orchestrator resolves the default VPC network.
- One dedicated gateway subnet named `vpngw-subnet` (CIDR /27) created if missing under the selected network; workload subnets remain separate.

**Current Platform Limitation (MVP):**
- Each gateway VM supports **1 NIC only** (platform constraint).
- The single NIC attaches to `vpngw-subnet` with **1 public IP allocation**.
- **All VPN tunnels share the same public IP**; peers differentiate tunnels by IKE/IPsec identifiers (phase1-id, phase2-id).
- This enables active/active tunnel redundancy while running on a single VM (VM remains SPOF).

**Future Migration Path (Multi-VM HA):**
- Deploy `instance_count=2` (or more) VMs.
- Each VM gets 1 NIC with 1 public IP allocation.
- Map one tunnel per VM using `gateway_instance_index` in tunnel config.
- Assign `ha_role: active|standby` to control which tunnels are active per VM.
- Use route controller to update VPC route next-hop on VM failure (failover from active to standby VM).
- This provides VM-level redundancy with tunnel isolation across instances.

**Route tables** are per subnet: `destination → next_hop` (allocation or default egress). No ECMP in route tables (one next hop per prefix). Load‑sharing happens within the gateway (FRR across tunnels) and/or on the peer side via BGP/ECMP.
- v1 does not use an external NAT/LB.

3) Gateway VM Architecture
VM spec (expressed in YAML, mapped by orchestrator)
- Platform, cores, memory, disk, base image (Ubuntu 22.04 LTS).
- Networking: two NICs on `vpngw-subnet` (created/ensured), each with exactly one public IP allocation attached. Private IPs are auto-assigned by the subnet.

Packages & services
- strongSwan (>=5.9), FRR (>=9), Python 3.x.
- `nebius-vpngw-agent` as a systemd service (continuous daemon, SIGHUP reload).
- Bootstrap via cloud‑init (packages, unit file, directory skeleton) — tunnel config is applied at runtime by the agent.

4) Configuration Model (YAML)
Single user‑facing file: `nebius-vpngw-config.yaml`.
- Sections: `gateway_group`, `defaults`, `gateway`, `connections[*].tunnels[*]` with `ha_role`, `routing_mode`, crypto proposals, and APIPA inner IPs.
- Networking fields:
  - `network_id` (optional): target VPC network. If omitted, defaults to the environment’s default network.
  - Implicit `vpngw-subnet` (/27): orchestrator ensures it exists and attaches both NICs there.
  - `external_ips`: optional list of public IP allocation IDs. If missing or empty, the orchestrator creates two allocations and attaches them to eth0/eth1.
- Merge precedence when building the resolved config:
  1. Tunnel‑level explicit values
  2. Connection‑level values
  3. Peer‑config values (from `--peer-config-file`)
  4. Global defaults
- Mandatory fields missing after merge → hard error with clear message.
- Environment placeholders `${VAR}` are expanded; missing vars are reported together.

5) CLI & Flows
Command
- Console script: `nebius-vpngw` (also available as `python -m nebius_vpngw`).
- First‑run convenience: if `--local-config-file` is omitted and `./nebius-vpngw-config.yaml` is absent, the CLI copies a packaged template into CWD and exits — edit it, then re‑run.

Options
- `--local-config-file PATH` (required unless auto‑created on first run)
- `--peer-config-file PATH` (repeatable)
- `--recreate-gw` (delete and recreate VMs before apply)
- `--project-id`, `--zone` (for Nebius SDK integration)
- `--dry-run` (show resolved summary without applying)

Typical flow
1. Parse args; load YAML; expand `${VAR}`; parse 0..N peer configs.
2. Merge to a resolved deployment plan; validate constraints; normalize `external_ips` to `[]` when absent.
3. Ensure network selection (`network_id` or default) and create/ensure `vpngw-subnet` (/27).
4. Ensure gateway VMs (create or reuse; `--recreate-gw` forces replacement) with two NICs on `vpngw-subnet`; create/attach two public IP allocations when not provided.
5. For each VM, build a per‑VM config and push via SSH, then `systemctl reload nebius-vpngw-agent`.
6. If enabled, reconcile static routes in VPC.

6) Peer Config Import (GCP/AWS/Azure/Cisco)
- Parser modules normalize vendor templates into per‑tunnel specs: PSK, inner IPs/CIDR, remote ASN, proposals, public endpoints.
- The merger auto‑matches peer tunnels to YAML tunnels using vendor, ASN, local/remote IP hints, and public IP alignment.
- Only missing values are filled; topology expressed in YAML is never overridden.

7) Routing Modes
- Global default: `defaults.routing.mode` = `bgp` or `static`; can be overridden per connection/tunnel.
- BGP: agent renders `bgpd.conf` with neighbors per active tunnel and advertises `gateway.local_prefixes` (timers/limits configurable).
- Static: agent skips BGP and installs static routes via the tunnel on the VM; peer must mirror static routes.

8) HA & SLA Modes
- Single VM (instance_count=1): multiple active tunnels; ECMP within the VM and/or on peer side; VM is SPOF.
- Multi‑VM (instance_count>1): assign tunnels via `gateway_instance_index`; `ha_role: active|disable` selects live tunnels per VM. HA controller can switch VPC routes to the standby VM on failure.

9) Idempotency & Lifecycle
Bootstrap vs runtime
- Cloud‑init: immutable basics only (packages, unit, dirs). No tunnel config in cloud‑init.
- Runtime: orchestrator can be re‑run any time; agent compares desired config with `last-applied.json` and only reloads when it changed.

VM lifecycle
- Lookup by deterministic name (`<group>-<index>`). `--recreate-gw` deletes/recreates; otherwise create‑if‑missing and push config.

Route reconciliation
- Idempotent ensure of VPC routes for static mode; skip for BGP mode.

10) State Management (practical guidance)
Question: “Should we keep and track state, or just apply YAML changes?”

Recommendation
- Keep it simple and safe: use minimal, VM‑local state in the agent (already implemented as `/etc/nebius-vpngw/last-applied.json`) to avoid unnecessary reloads. Keep the orchestrator effectively stateless: derive desired state from YAML + live discovery (by names) each run.

Why
- Pros of minimal state: idempotent applies, no tunnel flapping, restart‑safe, easy debugging of “last applied”.
- Avoid heavy external state: resource drift is best handled by reconciling actuals vs desired on every run; names are sufficient keys for Nebius resources.

When additional state may help
- Large fleets where caching resource IDs significantly reduces API calls (keep as an optimization cache, not source of truth).
- Advanced HA logic (tracking last healthy VM) — still prefer deriving from live signals.

Bottom line
- Keep the VM‑local state for last‑applied config (agent). Make the orchestrator reconcile from YAML + discovery. This matches the current code and keeps operations lean and predictable.

11) Implementation Status (this repo)
- CLI (`src/nebius_vpngw/cli.py`) and config merger (`config_loader.py`) are functional, including env expansion and peer‑merge heuristics.
- Agent (`agent/main.py`) renders scaffold configs for strongSwan/FRR and persists `last-applied.json`.
- Managers under `deploy/` (VM, SSH, routes) are scaffolds that log intended actions; integrate Nebius SDK and SSH to make them operational.
- Entry points are defined in `pyproject.toml`: `nebius-vpngw`, `nebius-vpngw-agent`, `build-binary`.
- First‑run template creation is implemented: run `nebius-vpngw` without `--local-config-file` to scaffold `./nebius-vpngw-config.yaml`.

Delta (current iteration)
- Networking model consolidated:
  - Single VPC network via `network_id` (optional) or default.
  - Orchestrator ensures `vpngw-subnet` (/27) exists under the selected network.
  - Two NICs per VM attached to `vpngw-subnet`, each with one public IP allocation created/attached when not provided.
- Config loader normalizes `external_ips=None` to `[]` to avoid errors.
- SSH key handling clarified: use explicit path fields; private key path is optional for local SSH convenience; never store private key contents in YAML.
- CLI and templates updated to reflect `network_id` and the single‑subnet, two‑NIC model.

12) Diagrams & References
- Architecture diagram sources: `image/vpngw-architecture.dot`, `image/vpngw-conn-diagram.dot` (render with Graphviz).
- See `README.md` for Quick Start, packaging, and build instructions. Run Poetry from the repository root (pyproject at top level).


