Nebius VPN Gateway (VM-based) — Architecture & Design

Version: v0.1 (scaffold-aligned)

Summary
- VM-based Site‑to‑Site VPN gateway for Nebius AI Cloud.
- IPsec (strongSwan) + BGP (FRR) or static routing.
- YAML‑driven desired state with optional peer config import (GCP/AWS/Azure/Cisco).
- ONE idempotent agent daemon per gateway VM; orchestrator CLI applies changes.

1) High‑Level Architecture
Components
- Orchestrator CLI (`nebius-vpngw`): runs on operator laptop/CI. Reads the local YAML and optional peer configs, merges to a resolved plan, ensures VMs, pushes per‑VM configs over SSH, and triggers the agent reload.
- Gateway VM (Nebius Compute): Ubuntu LTS base; packages: strongSwan, FRR, Python runtime, and the always‑running agent.
- Agent (`nebius-vpngw-agent`): **ONE daemon process per VM** that reads the per‑VM resolved config, renders strongSwan/FRR configs, applies them idempotently, and records last‑applied state (see Section 14 for detailed architecture).
- Optional HA Route Controller: updates VPC route next‑hop to switch between gateways when using multi‑VM setups.

Goals
- IKEv2 (default) with optional IKEv1; PSK auth; AES‑256; SHA‑256/384/512; DH 14/20/24; configurable lifetimes.
- BGP preferred; static routing supported per tunnel/connection.
- Works with GCP HA VPN, AWS Site‑to‑Site VPN, Azure VPN Gateway, and on‑prem routers.
- Two deployment modes:
  - Single VM with multiple tunnels (active/active at tunnel level; VM is SPOF).
  - Gateway group (N VMs) with route switching for VM‑level HA.

Current code alignment
- Package layout under `src/nebius_vpngw` matches design (cli, config loader, peer parsers, deploy managers, agent with state store/renderer stubs).
- VM/route/SSH managers are scaffolds; replace with Nebius SDK and SSH implementations to make it fully functional.

2) Nebius Topology & Routing Model
- Single VPC network selected by `network_id` (optional). If omitted, the orchestrator resolves the default VPC network.
- One dedicated gateway subnet named `vpngw-subnet` (CIDR /27) created if missing under the selected network; workload subnets remain separate.

**Current Platform Limitation (MVP):**
- Each gateway VM supports **1 NIC only** (platform constraint).
- The single NIC attaches to `vpngw-subnet` with **1 public IP allocation**.
- **All VPN tunnels share the same public IP**; peers differentiate tunnels by IKE/IPsec identifiers (phase1-id, phase2-id).
- This enables active/active tunnel redundancy while running on a single VM (VM remains SPOF).

**Configuration (num_nics):**
- YAML config includes `num_nics: 1` under `vm_spec` (default: 1, enforced by platform).
- When platform supports multi-NIC, this can be increased (e.g., `num_nics: 2`).
- Allocations are created 1 per NIC with naming: `{instance}-eth0-ip`, `{instance}-eth1-ip`, etc.
- Code is future-proofed to support multi-NIC when platform capabilities expand.

**Public IP Allocation (external_ips):**
- Configured via `gateway_group.external_ips` — a **nested array indexed by instance and NIC**.
- Structure: `external_ips[instance_index][nic_index]` → allocation IP string
- Examples:
  * Single VM, single NIC: `external_ips: [["1.2.3.4"]]` → instance 0, eth0
  * Multi-VM HA (2 VMs, 1 NIC each): `external_ips: [["1.2.3.4"], ["5.6.7.8"]]` → instance 0 eth0, instance 1 eth0
  * Future multi-NIC (1 VM, 2 NICs): `external_ips: [["1.2.3.4", "1.2.3.5"]]` → instance 0 eth0 and eth1
  * Future multi-VM + multi-NIC: `external_ips: [["1.2.3.4", "1.2.3.5"], ["5.6.7.8", "5.6.7.9"]]`
- Behavior:
  * If omitted/empty: Auto-create allocations for all instances (1 per NIC per instance)
  * If provided: Use existing allocation IPs (must exist in project under vpngw-subnet)
  * If insufficient: Auto-create for missing instances/NICs
- Preservation: Allocations are preserved on VM recreation (see Section 16)
- Naming: Auto-created allocations named `{instance_name}-eth{N}-ip` (e.g., `nebius-vpn-gw-0-eth0-ip`)

**Future Migration Path (Multi-NIC on Single VM):**
- When platform supports multi-NIC (e.g., `num_nics=2`):
  - Each VM gets 2 NICs (eth0, eth1) attached to `vpngw-subnet`.
  - Each NIC gets 1 public IP allocation.
  - Tunnels can be mapped to specific NICs for redundancy (tunnel 0 → eth0, tunnel 1 → eth1).
  - This provides NIC-level isolation within a single VM.

**Future Migration Path (Multi-VM HA):**
- Deploy `instance_count=2` (or more) VMs.
- Each VM gets `num_nics` NICs (currently 1) with corresponding public IPs.
- Map one tunnel per VM using `gateway_instance_index` in tunnel config.
- Assign `ha_role: active|standby` to control which tunnels are active per VM.
- Use route controller to update VPC route next-hop on VM failure (failover from active to standby VM).
- This provides VM-level redundancy with tunnel isolation across instances.

**Route tables** are per subnet: `destination → next_hop` (allocation or default egress). No ECMP in route tables (one next hop per prefix). Load‑sharing happens within the gateway (FRR across tunnels) and/or on the peer side via BGP/ECMP.
- v1 does not use an external NAT/LB.

3) Gateway VM Architecture
VM spec (expressed in YAML, mapped by orchestrator)
- Platform, cores, memory, disk, base image (Ubuntu 22.04 LTS).
- Networking: two NICs on `vpngw-subnet` (created/ensured), each with exactly one public IP allocation attached. Private IPs are auto-assigned by the subnet.

Packages & services
- strongSwan (>=5.9), FRR (>=9), Python 3.x.
- `nebius-vpngw-agent` as a systemd service (continuous daemon, SIGHUP reload). See Section 14 for detailed agent architecture.
- Bootstrap via cloud‑init (packages, unit file, directory skeleton) — tunnel config is applied at runtime by the agent.

4) Configuration Model (YAML)
Single user‑facing file: `nebius-vpngw-config.yaml`.
- Sections: `gateway_group`, `defaults`, `gateway`, `connections[*].tunnels[*]` with `ha_role`, `routing_mode`, crypto proposals, and APIPA inner IPs.
- Networking fields:
  - `network_id` (optional): target VPC network. If omitted, defaults to the environment’s default network.
  - Implicit `vpngw-subnet` (/27): orchestrator ensures it exists and attaches both NICs there.
  - `external_ips`: optional list of public IP allocation IDs. If missing or empty, the orchestrator creates two allocations and attaches them to eth0/eth1.
- Merge precedence when building the resolved config:
  1. Tunnel‑level explicit values
  2. Connection‑level values
  3. Peer‑config values (from `--peer-config-file`)
  4. Global defaults
- Mandatory fields missing after merge → hard error with clear message.
- Environment placeholders `${VAR}` are expanded; missing vars are reported together.

5) CLI & Flows
Command
- Console script: `nebius-vpngw` (also available as `python -m nebius_vpngw`).
- First‑run convenience: if `--local-config-file` is omitted and `./nebius-vpngw-config.yaml` is absent, the CLI copies a packaged template into CWD and exits — edit it, then re‑run.

Options
- `--local-config-file PATH` (required unless auto‑created on first run)
- `--peer-config-file PATH` (repeatable)
- `--recreate-gw` (delete and recreate VMs before apply)
- `--project-id`, `--zone` (for Nebius SDK integration)
- `--dry-run` (show resolved summary without applying)

Typical flow
1. Parse args; load YAML; expand `${VAR}`; parse 0..N peer configs.
2. Merge to a resolved deployment plan; validate constraints; normalize `external_ips` to `[]` when absent.
3. Ensure network selection (`network_id` or default) and create/ensure `vpngw-subnet` (/27).
4. Ensure gateway VMs (create or reuse; `--recreate-gw` forces replacement) with NICs on `vpngw-subnet`; attach public IP allocations per instance (indexed via `gateway_group.external_ips[instance_index][nic_index]`; preserved from previous VM or auto-created if unavailable). See Section 16 for allocation preservation strategy.
5. For each VM, build a per‑VM config and push via SSH, then `systemctl reload nebius-vpngw-agent`.
6. If enabled, reconcile static routes in VPC.

6) Peer Config Import (GCP/AWS/Azure/Cisco)
- Parser modules normalize vendor templates into per‑tunnel specs: PSK, inner IPs/CIDR, remote ASN, proposals, public endpoints.
- The merger auto‑matches peer tunnels to YAML tunnels using vendor, ASN, local/remote IP hints, and public IP alignment.
- Only missing values are filled; topology expressed in YAML is never overridden.

7) Routing Modes
- Global default: `defaults.routing.mode` = `bgp` or `static`; can be overridden per connection/tunnel.
- BGP: agent renders `bgpd.conf` with neighbors per active tunnel and advertises `gateway.local_prefixes` (timers/limits configurable).
- Static: agent skips BGP and installs static routes via the tunnel on the VM; peer must mirror static routes.

8) HA & SLA Modes
- Single VM (instance_count=1): multiple active tunnels; ECMP within the VM and/or on peer side; VM is SPOF.
- Multi‑VM (instance_count>1): assign tunnels via `gateway_instance_index`; `ha_role: active|disable` selects live tunnels per VM. HA controller can switch VPC routes to the standby VM on failure.

9) Idempotency & Lifecycle
Bootstrap vs runtime
- Cloud‑init: immutable basics only (packages, unit, dirs). No tunnel config in cloud‑init.
- Runtime: orchestrator can be re‑run any time; agent compares desired config with `last-applied.json` and only reloads when it changed.

VM lifecycle
- Lookup by deterministic name (`<group>-<index>`). `--recreate-gw` deletes/recreates VMs but preserves allocations; otherwise create‑if‑missing and push config. See Section 16 for allocation preservation details.

Route reconciliation
- Idempotent ensure of VPC routes for static mode; skip for BGP mode.

10) State Management (practical guidance)
Question: “Should we keep and track state, or just apply YAML changes?”

Recommendation
- Keep it simple and safe: use minimal, VM‑local state in the agent (already implemented as `/etc/nebius-vpngw/last-applied.json`) to avoid unnecessary reloads. Keep the orchestrator effectively stateless: derive desired state from YAML + live discovery (by names) each run.

Why
- Pros of minimal state: idempotent applies, no tunnel flapping, restart‑safe, easy debugging of “last applied”.
- Avoid heavy external state: resource drift is best handled by reconciling actuals vs desired on every run; names are sufficient keys for Nebius resources.

When additional state may help
- Large fleets where caching resource IDs significantly reduces API calls (keep as an optimization cache, not source of truth).
- Advanced HA logic (tracking last healthy VM) — still prefer deriving from live signals.

Bottom line
- Keep the VM‑local state for last‑applied config (agent). Make the orchestrator reconcile from YAML + discovery. This matches the current code and keeps operations lean and predictable.

11) Implementation Status (this repo)
- CLI (`src/nebius_vpngw/cli.py`) and config merger (`config_loader.py`) are functional, including env expansion and peer‑merge heuristics.
- Agent (`agent/main.py`) renders scaffold configs for strongSwan/FRR and persists `last-applied.json`.
- Managers under `deploy/` (VM, SSH, routes) are scaffolds that log intended actions; integrate Nebius SDK and SSH to make them operational.
- Entry points are defined in `pyproject.toml`: `nebius-vpngw`, `nebius-vpngw-agent`, `build-binary`.
- First‑run template creation is implemented: run `nebius-vpngw` without `--local-config-file` to scaffold `./nebius-vpngw-config.yaml`.

Delta (current iteration)
- Networking model consolidated:
  - Single VPC network via `network_id` (optional) or default.
  - Orchestrator ensures `vpngw-subnet` (/27) exists under the selected network, if not has to be created.
  - Two NICs per VM attached to `vpngw-subnet`, each with one public IP allocation created/attached when not provided.
- Config loader normalizes `external_ips=None` to `[]` to avoid errors.
- SSH key handling clarified: use explicit path fields; private key path is optional for local SSH convenience; never store private key contents in YAML.
- CLI and templates updated to reflect `network_id` and the single‑subnet, two‑NIC model.

12) Diagrams & References
- Architecture diagram sources: `image/vpngw-architecture.dot`, `image/vpngw-conn-diagram.dot` (render with Graphviz).
- See `README.md` for Quick Start, packaging, and build instructions. Run Poetry from the repository root (pyproject at top level).

Limitations:
The platform has two limitations:
- Only 1 NIC per instance
- Aliases only support private IPs, not public allocations

13) Local Prefixes Advertisement Strategy
Overview
- The VPN gateway advertises Nebius-side subnets (local prefixes) to remote peers via BGP or static routes.
- A hierarchical configuration model ensures consistency, clarity, and flexibility across different routing modes and use cases.

Design Principles
1. Single Source of Truth: Gateway-level `local_prefixes` defines all Nebius-side subnets globally.
2. Mode-Specific Control: Connection and tunnel levels provide fine-grained control based on routing mode.
3. Override Capability: Tunnel-level configuration can override defaults for specialized scenarios.
4. Clear Semantics: Each level has a distinct purpose to avoid confusion and misconfiguration.

Configuration Hierarchy
┌─────────────────────────────────────────────────────────────────┐
│ Level 1: Gateway (gateway.local_prefixes)                      │
│ Purpose: Define ALL Nebius-side subnets (single source)        │
│ Example: local_prefixes: [10.0.0.0/16, 172.16.0.0/12]         │
│ Used By: Both BGP and Static routing modes                      │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ Level 2A: BGP Connection (connection.bgp.advertise_local_...)  │
│ Purpose: Control whether to advertise gateway prefixes via BGP │
│ Example: advertise_local_prefixes: true                        │
│ Behavior:                                                       │
│   - true  → Advertise all gateway.local_prefixes via BGP       │
│   - false → Do NOT advertise any prefixes (BGP neighbor only)  │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│ Level 2B: Static Tunnel (tunnel.static_routes.local_prefixes)  │
│ Purpose: Override gateway defaults for tunnel-specific routing │
│ Example: local_prefixes: [10.0.1.0/24]                         │
│ Behavior:                                                       │
│   - Omitted or [] → Uses gateway.local_prefixes (default)      │
│   - Specified     → Overrides gateway.local_prefixes for this  │
│                     tunnel only (subnet isolation, migration)  │
└─────────────────────────────────────────────────────────────────┘

Routing Mode Behaviors

BGP (Dynamic Routing)
- Source: gateway.local_prefixes (always)
- Control: connection.bgp.advertise_local_prefixes (true/false)
- Process:
  1. FRR renderer extracts gateway_local_prefixes from gateway config
  2. For each BGP connection, checks advertise_local_prefixes flag
  3. If true, accumulates gateway_local_prefixes into advertised_prefixes set
  4. Generates BGP "network" statements for all accumulated prefixes
  5. BGP advertises these prefixes to all neighbors in the connection
- Override: No tunnel-level override (BGP operates at connection level)
- Use Cases:
  - Multi-peer BGP with selective advertisement per connection
  - BGP peering without route advertisement (advertise_local_prefixes: false)
  - Consistent prefix advertisement across all tunnels in a connection

Static Routing (Policy-Based IPsec)
- Source: gateway.local_prefixes (default) OR tunnel.static_routes.local_prefixes (override)
- Control: Per-tunnel configuration
- Process:
  1. strongSwan renderer extracts gateway_local_prefixes from gateway config
  2. For each static tunnel, checks tunnel.static_routes.local_prefixes
  3. If omitted/empty, uses gateway_local_prefixes
  4. If specified, uses tunnel.static_routes.local_prefixes (override)
  5. Generates IPsec leftsubnet configuration with selected prefixes
- Override: Tunnel-level local_prefixes overrides gateway defaults
- Use Cases:
  - Default: All tunnels advertise all gateway.local_prefixes
  - Subnet isolation: Different tunnels advertise different subnet ranges
  - Migration: Gradually shift prefixes from one tunnel to another
  - Multi-region: Tunnel-specific prefixes for geographically isolated subnets

Configuration Examples

Example 1: BGP with Multiple Connections (Selective Advertisement)
```yaml
gateway:
  local_prefixes: [10.0.0.0/16, 172.16.0.0/12]  # All Nebius subnets
  
connections:
  - name: gcp-prod
    routing_mode: bgp
    bgp:
      advertise_local_prefixes: true   # Advertise all gateway prefixes
      local_asn: 64512
      remote_asn: 64513
    tunnels: [...]
    
  - name: gcp-test
    routing_mode: bgp
    bgp:
      advertise_local_prefixes: false  # BGP peer only, no route ads
      local_asn: 64512
      remote_asn: 64514
    tunnels: [...]
```
Result:
- gcp-prod: Advertises 10.0.0.0/16 and 172.16.0.0/12 via BGP
- gcp-test: BGP neighbor established, NO prefixes advertised

Example 2: Static with Tunnel-Specific Overrides (Subnet Isolation)
```yaml
gateway:
  local_prefixes: [10.0.0.0/16]  # Default for all static tunnels
  
connections:
  - name: cisco-datacenter
    routing_mode: static
    tunnels:
      - name: tunnel-0
        # Omit static_routes.local_prefixes → uses gateway defaults
        # Result: leftsubnet=10.0.0.0/16
        
      - name: tunnel-1
        static_routes:
          local_prefixes: [10.0.1.0/24]  # Override for this tunnel
        # Result: leftsubnet=10.0.1.0/24 (subnet isolation)
```
Result:
- tunnel-0: Advertises entire 10.0.0.0/16
- tunnel-1: Advertises only 10.0.1.0/24 (isolated subnet)

Example 3: Mixed BGP and Static (Multi-Mode)
```yaml
gateway:
  local_prefixes: [10.0.0.0/16, 172.16.0.0/12]
  
connections:
  - name: gcp-bgp
    routing_mode: bgp
    bgp:
      advertise_local_prefixes: true
    # Result: BGP advertises 10.0.0.0/16 and 172.16.0.0/12
    
  - name: cisco-static
    routing_mode: static
    tunnels:
      - name: tunnel-0
        # Uses gateway.local_prefixes (default)
        # Result: leftsubnet=10.0.0.0/16,172.16.0.0/12
```
Result:
- GCP BGP: Dynamic advertisement of both prefixes
- Cisco Static: Policy-based IPsec with both prefixes

Comparison Matrix
┌───────────────┬─────────────────────┬──────────────────────┬─────────────────┐
│ Routing Mode  │ Source Level        │ Control Mechanism    │ Override?       │
├───────────────┼─────────────────────┼──────────────────────┼─────────────────┤
│ BGP           │ Gateway             │ Connection flag      │ No              │
│               │ (local_prefixes)    │ (advertise_local_*)  │                 │
├───────────────┼─────────────────────┼──────────────────────┼─────────────────┤
│ Static        │ Gateway (default)   │ Tunnel-level         │ Yes (per tunnel)│
│               │ Tunnel (override)   │ (local_prefixes)     │                 │
└───────────────┴─────────────────────┴──────────────────────┴─────────────────┘

Implementation Details

File: nebius-vpngw-config-template.yaml
- Gateway section documents local_prefixes as "single source of truth"
- Connection section explains advertise_local_prefixes for BGP control
- Tunnel section shows static_routes.local_prefixes override behavior
- Comprehensive header documentation (35 lines) with examples and use cases

File: agent/frr_renderer.py
- Extracts gateway_local_prefixes from gateway configuration
- Iterates through connections and respects advertise_local_prefixes flag
- Accumulates prefixes into advertised_prefixes set when flag is true
- Generates BGP "network" statements for all accumulated prefixes

File: agent/strongswan_renderer.py
- Extracts gateway_local_prefixes from gateway configuration
- For static tunnels, checks tunnel.static_routes.local_prefixes
- If tunnel prefixes omitted/empty, uses gateway_local_prefixes (default)
- If tunnel prefixes specified, uses tunnel values (override)
- Generates leftsubnet in ipsec.conf with selected prefixes

File: config_loader.py
- Validates gateway.local_prefixes is present and non-empty
- Merges connection-level and tunnel-level configurations
- Provides defaults for advertise_local_prefixes (true for BGP)
- Preserves tunnel-level static_routes.local_prefixes when specified

Validation Rules
1. Gateway.local_prefixes is mandatory and must be non-empty
2. BGP: advertise_local_prefixes defaults to true if omitted
3. Static: tunnel.static_routes.local_prefixes is optional (defaults to gateway)
4. Tunnel-level local_prefixes only applies to static routing mode
5. All CIDR notations must be valid (e.g., 10.0.0.0/16)

Best Practices
1. Define all Nebius subnets in gateway.local_prefixes once (DRY principle)
2. Use advertise_local_prefixes: false for BGP peering without route ads
3. Omit tunnel.static_routes.local_prefixes unless you need subnet isolation
4. Document tunnel-specific overrides clearly in YAML comments
5. Use consistent CIDR notation across all prefix configurations
6. Test prefix advertisement with BGP show commands (show ip bgp neighbors)
7. Verify static routes with show ip route on both gateway and peer

Migration Scenarios
- Adding new subnet: Update gateway.local_prefixes only (all connections inherit)
- Isolating subnet: Add tunnel.static_routes.local_prefixes override
- BGP to Static: Remove bgp section, add static_routes with defaults
- Static to BGP: Remove static_routes overrides, add bgp section with flag

Benefits
1. Clarity: Single source of truth eliminates configuration duplication
2. Flexibility: Tunnel-level overrides support advanced use cases
3. Consistency: Same gateway.local_prefixes used across all modes
4. Maintainability: Centralized prefix management reduces errors
5. Scalability: Easy to add/remove subnets without touching all tunnels
6. Debuggability: Clear hierarchy makes troubleshooting straightforward

14) Agent Architecture (On-VM Components)

Overview
Each gateway VM runs **ONE agent** (`nebius-vpngw-agent`) as a single systemd service/daemon. This agent is responsible for:
1. Reading the resolved configuration pushed by the orchestrator
2. Detecting configuration changes (via hash comparison)
3. Rendering strongSwan and FRR configuration files
4. Applying the configurations to the IPsec and BGP services
5. Persisting the last-applied state to avoid unnecessary service restarts

Agent Count: 1 per VM
- Process name: `nebius-vpngw-agent`
- Systemd unit: `nebius-vpngw-agent.service`
- Entry point: `src/nebius_vpngw/agent/main.py`
- Lifecycle: Continuous daemon that listens for SIGHUP signals for reload

Architecture
┌─────────────────────────────────────────────────────────────────┐
│                    nebius-vpngw-agent (ONE process)             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ Main Controller (agent/main.py)                          │  │
│  │ • Loads /etc/nebius-vpngw/config-resolved.yaml          │  │
│  │ • Compares hash against last-applied state              │  │
│  │ • Orchestrates renderers if config changed              │  │
│  │ • Persists new state after successful apply             │  │
│  │ • Listens for SIGHUP to trigger reload                  │  │
│  └──────────────────────────────────────────────────────────┘  │
│                           │                                     │
│                           ├─────────────────┬──────────────┐   │
│                           ▼                 ▼              ▼   │
│  ┌────────────────┐  ┌───────────────┐  ┌──────────────┐     │
│  │ StateStore     │  │ StrongSwan    │  │ FRR          │     │
│  │ (state_store.  │  │ Renderer      │  │ Renderer     │     │
│  │ py)            │  │ (strongswan_  │  │ (frr_        │     │
│  │                │  │ renderer.py)  │  │ renderer.py) │     │
│  │ • SHA256 hash  │  │               │  │              │     │
│  │ • last-applied │  │ • Renders     │  │ • Renders    │     │
│  │   .json        │  │   ipsec.conf  │  │   bgpd.conf  │     │
│  │ • is_changed() │  │ • Renders     │  │ • BGP        │     │
│  │ • save_last_   │  │   ipsec.      │  │   neighbors  │     │
│  │   applied()    │  │   secrets     │  │ • Network    │     │
│  │                │  │ • Handles     │  │   statements │     │
│  │                │  │   VTI & static│  │ • Prefix     │     │
│  │                │  │   routing     │  │   ads        │     │
│  └────────────────┘  │ • IKEv1/IKEv2 │  │              │     │
│                      │ • DPD config  │  │              │     │
│                      └───────────────┘  └──────────────┘     │
└─────────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│            External Services (managed by agent)                  │
├─────────────────────────────────────────────────────────────────┤
│  • strongSwan (ipsec daemon) - IPsec tunnels                    │
│  • FRR (frr/bgpd) - BGP routing                                 │
└─────────────────────────────────────────────────────────────────┘

Component Responsibilities

1. Main Controller (agent/main.py)
   - Entry point: `main()` function (called by systemd)
   - Initialization: Creates Agent() instance with StateStore, StrongSwanRenderer, FRRRenderer
   - Config loading: Reads /etc/nebius-vpngw/config-resolved.yaml (pushed by orchestrator via SSH)
   - Change detection: Calls StateStore.is_changed(cfg) to compare SHA256 hash
   - Conditional rendering: Only calls renderers if config hash differs from last-applied
   - State persistence: Calls StateStore.save_last_applied(cfg) after successful apply
   - Signal handling: Registers SIGHUP handler to trigger reload() when orchestrator pushes new config
   - Lifecycle: Runs reload() once on startup, then enters signal.pause() waiting for SIGHUP

2. StateStore (agent/state_store.py)
   - Purpose: Hash-based change detection and state persistence
   - File: /etc/nebius-vpngw/last-applied.json
   - Methods:
     * load_last_applied() → dict | None: Loads persisted state from JSON
     * _hash_cfg(resolved_config) → str: Computes SHA256 hash of config (deterministic)
     * is_changed(resolved_config) → bool: Compares new hash vs last-applied hash
     * save_last_applied(resolved_config) → None: Persists config + hash + timestamp
   - Benefits:
     * Prevents unnecessary strongSwan/FRR reloads (avoids tunnel flapping)
     * Idempotent: Orchestrator can push same config repeatedly without disruption
     * Performance: Service reload is expensive; hash comparison is cheap

3. StrongSwanRenderer (agent/strongswan_renderer.py)
   - Purpose: Render IPsec tunnel configuration
   - Output files:
     * /etc/ipsec.conf (tunnel definitions)
     * /etc/ipsec.secrets (PSK credentials)
   - Responsibilities:
     * Parse connections[] and tunnels[] from resolved config
     * Filter active tunnels (ha_role="active")
     * Generate IPsec connection stanzas with:
       - IKEv1/IKEv2 keyexchange
       - PSK authentication
       - Crypto proposals (IKE/ESP algorithms, lifetimes)
       - DPD (Dead Peer Detection) settings
       - Local/remote endpoints (left/right)
       - VTI mode (for BGP): leftsubnet/rightsubnet = inner_cidr, mark for VTI interface
       - Static mode: leftsubnet = gateway.local_prefixes (or tunnel override), rightsubnet = remote_prefixes
     * Write ipsec.conf and ipsec.secrets to disk
     * Note: Actual reload of strongSwan is commented (production: `ipsec reload` or `systemctl restart strongswan-starter`)
   - Routing mode handling:
     * BGP (routing_mode="bgp"): Uses VTI with APIPA inner IPs, mark for tunnel interface
     * Static (routing_mode="static"): Uses policy-based IPsec with subnet selectors

4. FRRRenderer (agent/frr_renderer.py)
   - Purpose: Render BGP routing configuration
   - Output file: /etc/frr/bgpd.conf
   - Responsibilities:
     * Parse connections[] and tunnels[] from resolved config
     * Filter BGP connections (routing_mode="bgp")
     * Extract gateway.local_prefixes as single source of truth for local subnets
     * Generate BGP configuration with:
       - Router BGP section with local ASN
       - Keepalive/holdtime timers
       - Graceful restart (if enabled)
       - Neighbor statements per active tunnel (remote_ip, remote_asn)
       - Maximum prefix limits per neighbor
       - Network statements for prefix advertisement (gateway.local_prefixes when advertise_local_prefixes=true)
     * Accumulates advertised_prefixes set across connections respecting advertise_local_prefixes flag
     * Write bgpd.conf to disk
     * Note: Actual reload of FRR is commented (production: `systemctl reload frr` or vtysh commands)
   - Control flags:
     * connection.bgp.advertise_local_prefixes (true/false): Controls whether to advertise gateway prefixes

Workflow Example

Initial Apply (VM bootstrap):
1. Orchestrator creates VM, installs packages, sets up systemd unit
2. Orchestrator pushes /etc/nebius-vpngw/config-resolved.yaml via SSH
3. Orchestrator triggers: `systemctl reload nebius-vpngw-agent` (or `kill -HUP`)
4. Agent receives SIGHUP → calls reload()
5. StateStore.is_changed() returns true (no last-applied.json yet)
6. StrongSwanRenderer.render_and_apply() → writes /etc/ipsec.conf, /etc/ipsec.secrets
7. FRRRenderer.render_and_apply() → writes /etc/frr/bgpd.conf
8. StateStore.save_last_applied() → persists config + hash to /etc/nebius-vpngw/last-applied.json
9. Agent prints: "[Agent] Applied and persisted new configuration"
10. Agent returns to signal.pause() waiting for next SIGHUP

Subsequent Apply (config change):
1. User edits nebius-vpngw-config.yaml (e.g., changes PSK, adds tunnel)
2. User runs: `nebius-vpngw apply`
3. Orchestrator detects SAFE changes (no VM recreation needed)
4. Orchestrator pushes updated /etc/nebius-vpngw/config-resolved.yaml via SSH
5. Orchestrator triggers: `systemctl reload nebius-vpngw-agent`
6. Agent receives SIGHUP → calls reload()
7. StateStore.is_changed() compares SHA256 hash:
   - Old hash: abc123... (from last-applied.json)
   - New hash: def456... (from new config)
   - Returns: true (config changed)
8. StrongSwanRenderer.render_and_apply() → updates ipsec.conf with new PSK
9. FRRRenderer.render_and_apply() → updates bgpd.conf with new tunnel
10. StateStore.save_last_applied() → updates last-applied.json with new hash
11. strongSwan and FRR reload their configs (tunnels re-establish with new settings)
12. Agent prints: "[Agent] Applied and persisted new configuration"

No-Change Apply (idempotent):
1. User runs: `nebius-vpngw apply` again (no YAML changes)
2. Orchestrator pushes same /etc/nebius-vpngw/config-resolved.yaml
3. Orchestrator triggers: `systemctl reload nebius-vpngw-agent`
4. Agent receives SIGHUP → calls reload()
5. StateStore.is_changed() compares SHA256 hash:
   - Old hash: def456...
   - New hash: def456... (identical)
   - Returns: false (no changes)
6. Agent prints: "[Agent] No changes detected; skipping apply"
7. Renderers are NOT called (strongSwan/FRR not touched)
8. Tunnels remain stable (no flapping, no disruption)
9. Agent returns to signal.pause()

Key Design Decisions

Single Agent vs Multiple Daemons
- Decision: ONE agent process manages both strongSwan and FRR rendering
- Rationale:
  * Simplicity: Single systemd unit, single reload signal, single state file
  * Consistency: Atomic config changes across both IPsec and BGP
  * Performance: One hash check instead of multiple per-service checks
- Alternative (rejected): Separate agents for strongSwan and FRR would require:
  * Two systemd units, two state files, two reload signals
  * Coordination challenges (race conditions, partial applies)
  * More complexity for marginal benefit

Hash-Based Change Detection
- Decision: SHA256 hash of full resolved config in StateStore
- Rationale:
  * Fast: Hash comparison is O(1) vs full config diff O(n)
  * Reliable: Cryptographic hash ensures no false negatives
  * Simple: No need to track field-by-field changes in agent
- Alternative (rejected): Field-by-field diff in agent would require:
  * Complex comparison logic duplicating orchestrator's vm_diff.py
  * More code to maintain, more bugs to fix
  * No performance benefit (config is small, hash is cheap)

Full Config Push vs Incremental Updates
- Decision: Orchestrator always pushes full resolved config
- Rationale:
  * Declarative: Agent sees complete desired state, not deltas
  * Idempotent: Same input → same output (no accumulated state bugs)
  * Debuggable: /etc/nebius-vpngw/config-resolved.yaml is complete and readable
- Agent optimization: Hash check prevents unnecessary service reloads
- See Section 15 for orchestrator-side rationale (SSH push strategy)

File Locations
- Config (input): /etc/nebius-vpngw/config-resolved.yaml (pushed by orchestrator)
- State (output): /etc/nebius-vpngw/last-applied.json (persisted by agent)
- strongSwan: /etc/ipsec.conf, /etc/ipsec.secrets (rendered by agent)
- FRR: /etc/frr/bgpd.conf (rendered by agent)

Integration with External Services
- strongSwan: Agent writes config files, then service reloads them
  * Production command: `ipsec reload` or `systemctl restart strongswan-starter`
  * Currently commented in code (manual testing phase)
- FRR: Agent writes bgpd.conf, then service reloads it
  * Production command: `systemctl reload frr` or `vtysh` commands
  * Currently commented in code (manual testing phase)

Summary
- **Agent count**: 1 per VM (single daemon process)
- **Main controller**: agent/main.py (loads config, detects changes, orchestrates renderers)
- **State management**: agent/state_store.py (SHA256 hash, last-applied persistence)
- **IPsec rendering**: agent/strongswan_renderer.py (ipsec.conf, ipsec.secrets)
- **BGP rendering**: agent/frr_renderer.py (bgpd.conf)
- **Trigger mechanism**: SIGHUP signal (via `systemctl reload` or `kill -HUP`)
- **Idempotency**: Hash comparison prevents unnecessary service reloads
- **Design philosophy**: Orchestrator pushes full config, agent decides if reload needed

15) Idempotency & Smart Change Detection

Strategy: Intelligent Diff-Based Reconciliation
- The orchestrator compares YAML (desired state) against live Nebius resources (actual state) on every run
- Changes are categorized as SAFE (apply via SSH) or DESTRUCTIVE (require VM recreation)
- Destructive changes are blocked unless user explicitly provides --recreate-gw flag
- Safe changes (tunnel configs, PSKs, routing) are pushed via SSH without disruption

Change Classification:
┌─────────────────────────────────────────────────────────────────────┐
│ DESTRUCTIVE (requires --recreate-gw)                                │
├─────────────────────────────────────────────────────────────────────┤
│ • platform (e.g., cpu-d3 → gpu-h100-sxm)                           │
│ • preset (e.g., 32vcpu-128gb → 64vcpu-256gb)                       │
│ • disk_boot_image (e.g., ubuntu22.04 → ubuntu24.04)                │
│ • disk_type (e.g., NETWORK_SSD → NETWORK_SSD_IO_M3)                │
│ • disk_block_bytes (e.g., 4096 → 16384)                            │
│ • disk_gb (shrinking only, e.g., 200GB → 100GB)                    │
│ • num_nics (shrinking only, e.g., 2 → 1)                           │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│ SAFE (applied via SSH config push or VM API attach)                │
├─────────────────────────────────────────────────────────────────────┤
│ • Tunnel configurations (remote IPs, inner IPs, phase IDs)         │
│ • PSK secrets                                                       │
│ • BGP settings (ASN, timers, prefixes)                             │
│ • Routing mode (bgp → static or vice versa)                        │
│ • local_prefixes (gateway/connection/tunnel levels)                │
│ • Crypto proposals (IKE/ESP algorithms, lifetimes, DPD)            │
│ • Connection-level settings (HA role, routing mode)                │
│ • disk_gb (expanding only, e.g., 100GB → 200GB)                    │
│ • num_nics (expanding only, e.g., 1 → 2 via NIC attach)            │
└─────────────────────────────────────────────────────────────────────┘

Implementation Components:
1. vm_diff.py module:
   - VMSpec: Normalizes VM configuration from YAML and live resources
   - VMDiffAnalyzer: Compares desired vs actual, categorizes changes
   - VMDiff: Result object with change type and detailed differences
   - ChangeType enum: NO_CHANGE, SAFE, DESTRUCTIVE

2. VMManager.check_changes():
   - Queries Nebius API for existing VMs and boot disks by name
   - Extracts live VMSpec from resource metadata
   - Compares against desired VMSpec from YAML
   - Returns list of (instance_name, VMDiff) tuples

3. CLI integration (cli.py):
   - Calls check_changes() before ensure_group()
   - Displays human-readable diff summary to user
   - Blocks destructive changes unless --recreate-gw provided
   - Allows safe changes to proceed automatically

User Workflow:
1. Edit nebius-vpngw-config.yaml (change tunnel configs, PSKs, routing)
2. Run: nebius-vpngw apply
3. Orchestrator analyzes changes:
   - Safe changes: "✓ Will apply via SSH config push"
   - Destructive changes: "⚠️  Requires --recreate-gw (data loss warning)"
4. Safe changes applied automatically (SSH → agent reload)
5. Destructive changes require explicit confirmation:
   - Run: nebius-vpngw apply --recreate-gw
   - User acknowledges VM deletion and recreation
   - Public IP allocations are preserved (not deleted) - see Section 16

Benefits:
- Idempotent: Run apply repeatedly with same result
- Safe by default: Prevents accidental VM deletion
- Clear feedback: User knows exactly what will change
- Flexible: Override protection with explicit flag when needed
- No external state: Derives everything from YAML + API discovery
- Minimal disruption: Safe changes applied without downtime

Example Scenarios:

Scenario 0: Initial VM creation (clean slate)
- Prerequisites: No VM exists, YAML configured
- Run: nebius-vpngw --local-config-file ./nebius-vpngw-config.min.yaml
  (or simply: nebius-vpngw if using default nebius-vpngw-config.yaml in CWD)
- Result: VM created (no --recreate-gw flag needed)
- Process: Subnet created → Allocation created → VM created → Config pushed
- Note: Initial creation is always safe, no flag required

Scenario 1: Update PSK for tunnel
- Edit YAML: change connection.tunnels[0].crypto.psk
- Run: nebius-vpngw --local-config-file ./nebius-vpngw-config.yaml
  (or: nebius-vpngw apply --local-config-file ./nebius-vpngw-config.yaml)
- Result: SSH push new config → agent reloads strongSwan → tunnel re-establishes
- No VM recreation, minimal disruption

Scenario 2: Add new BGP connection
- Edit YAML: add new connection with tunnels
- Run: nebius-vpngw
- Result: SSH push updated config → agent adds new tunnels → BGP peers up
- Existing tunnels unaffected

Scenario 3: Change platform (cpu-d3 → gpu-h100) - DESTRUCTIVE
- Edit YAML: vm_spec.platform = "gpu-h100-sxm"
- Run: nebius-vpngw
- Result: ⚠️  ERROR - Destructive change detected (platform change on existing VM)
- Run: nebius-vpngw --recreate-gw
- Result: VM deleted → new VM created → config pushed → tunnels re-establish
- Downtime expected and acknowledged by user

Scenario 4: Expand disk size
- Edit YAML: vm_spec.disk_gb = 300 (was 200)
- Run: nebius-vpngw apply
- Result: Safe change detected (disk expansion allowed)
- Note: Actual resize may require manual partition/filesystem resize on VM

Scenario 5: Add second NIC (when platform supports multi-NIC)
- Edit YAML: vm_spec.num_nics = 2 (was 1)
- Run: nebius-vpngw apply
- Result: Safe change detected (NIC expansion)
- Process:
  1. Create second public IP allocation (eth1-ip)
  2. Attach second NIC to existing VM (eth1)
  3. Assign allocation to new NIC
  4. Update routes/tunnels to use new NIC as needed
- No VM recreation, no downtime for existing tunnels on eth0

Scenario 6: Force recreation with --recreate-gw (no config changes)
- YAML unchanged (no modifications)
- Run: nebius-vpngw apply --recreate-gw
- Result: ⚠️  WARNING - No changes detected, recreation is unnecessary
- Prompt: "Do you want to proceed? [y/N]"
- If user confirms: VM deleted → VM recreated with identical specs → downtime
- If user declines: Aborted, no changes made
- Use case: Recovery from VM corruption, testing disaster recovery

Scenario 7: Safe changes with --recreate-gw (unnecessary but allowed)
- Edit YAML: change tunnel PSK (safe change)
- Run: nebius-vpngw apply --recreate-gw
- Result: Safe changes detected, but --recreate-gw forces VM recreation
- Process: VM deleted → VM recreated → config pushed with new PSK
- Note: Unnecessary downtime; should just run without --recreate-gw

--recreate-gw Flag Behavior Summary:
┌─────────────────────────┬───────────────┬──────────────────────────────┐
│ Changes Detected        │ Flag Provided │ Behavior                     │
├─────────────────────────┼───────────────┼──────────────────────────────┤
│ VM doesn't exist        │ No            │ Create VM (safe)             │
│ VM doesn't exist        │ Yes           │ Create VM (flag ignored)     │
│ None (no changes)       │ No            │ Skip VM ops, SSH push        │
│ None (no changes)       │ Yes           │ Warn + confirm prompt        │
│ Safe only               │ No            │ SSH push only (no VM)        │
│ Safe only               │ Yes           │ Recreate (unnecessary)       │
│ Destructive (VM exists) │ No            │ ERROR + abort                │
│ Destructive (VM exists) │ Yes           │ Recreate (required)          │
└─────────────────────────┴───────────────┴──────────────────────────────┘

Key Improvement:
- Initial VM creation is now treated as SAFE (not destructive)
- No --recreate-gw flag needed when VM doesn't exist
- Flag only required when deleting and recreating existing VMs
- This eliminates confusion for first-time deployments

Agent-Side State (unchanged):
- /etc/nebius-vpngw/last-applied.json persists config hash
- Agent compares incoming config against last-applied hash
- Skips strongSwan/FRR reload if config unchanged (avoids tunnel flapping)
- Orchestrator pushes config every time; agent decides whether to apply

This approach combines:
- Orchestrator-side: Smart diff analysis and protection
- Agent-side: Minimal state for reload optimization
- Result: Robust idempotency without complex state management

┌──────────────────────────────────────────────────────────┐
│                  YAML Configuration                      │
│              (Source of Truth - Desired)                 │
└────────────────────┬─────────────────────────────────────┘
                     │
                     ▼
┌──────────────────────────────────────────────────────────┐
│              VMDiffAnalyzer.compare()                    │
│  ┌─────────────────────────────────────────────────┐    │
│  │ VMSpec.from_config(yaml)  ←→  VMSpec.from_live() │    │
│  │     (desired)                    (actual)        │    │
│  └─────────────────────────────────────────────────┘    │
│                         ↓                                │
│  ┌─────────────────────────────────────────────────┐    │
│  │ Field-by-field comparison                        │    │
│  │ • platform? preset? disk_*? num_nics?           │    │
│  │ • Categorize: SAFE vs DESTRUCTIVE               │    │
│  └─────────────────────────────────────────────────┘    │
└────────────────────┬─────────────────────────────────────┘
                     │
                     ▼
        ┌────────────┴────────────┐
        │                         │
   DESTRUCTIVE                  SAFE
        │                         │
        ▼                         ▼
  Block unless              Apply via SSH
  --recreate-gw            (no VM recreation)

  DESTRUCTIVE (requires --recreate-gw):
├─ platform, preset, disk_boot_image
├─ disk_type, disk_block_bytes
├─ disk_gb (shrinking only)

SAFE (automatic via SSH):
├─ Tunnel configs, PSKs, crypto proposals
├─ BGP settings, routing mode
├─ local_prefixes (all levels)
└─ disk_gb (expanding only), num_nics

SSH Push Only Strategy
┌─────────────────────────────────────────────────────┐
│ Complete per-VM resolved configuration             │
├─────────────────────────────────────────────────────┤
│ • Gateway settings (local_prefixes, etc.)          │
│ • All connections (every connection defined)        │
│ • All tunnels (every tunnel with full config)      │
│ • All PSKs, crypto proposals, BGP settings         │
│ • Complete routing configuration                    │
│ • Everything the agent needs to render configs     │
└─────────────────────────────────────────────────────┘
                         ↓
           /etc/nebius-vpngw/config-resolved.yaml
                         ↓
                  Agent receives it
                         ↓
    Agent compares hash against last-applied.json
                         ↓
         ┌──────────────┴──────────────┐
         │                             │
    Hash matches                 Hash differs
         │                             │
         ↓                             ↓
  Skip reload                  Render new configs
  (no changes)                 ipsec.conf, bgpd.conf
                               ↓
                          Reload services
                          (strongSwan, FRR)

16) Public IP Allocation Lifecycle & Preservation

Overview
Public IP allocations in Nebius AI Cloud are **persistent resources** that can be reserved and reused across VM recreations. This is critical for VPN gateways because:
- Remote peers are configured with specific public IPs
- Recreating VMs should NOT require reconfiguring remote peers
- IP stability is essential for production VPN connectivity

Nebius Allocation Behavior
- **Reservation**: Allocations reserve a public IP address in a subnet
- **Assignment**: Allocations can be attached/detached from VM NICs
- **30-Day Grace Period**: Unassigned allocations are preserved for 30 days before potential cleanup
- **Explicit Deletion**: User must explicitly delete allocations to release them permanently

Design Strategy: Preserve Allocations on VM Recreation

Core Principle
When `--recreate-gw` deletes and recreates VMs, the orchestrator **MUST NOT delete allocations**. Instead:
1. Detach allocations from old VM NICs (automatic when VM deleted)
2. Leave allocations unassigned in the subnet (vpngw-subnet)
3. Reassign same allocations to new VM NICs
4. Preserve public IP addresses across the recreation

Implementation Workflow

VM Recreation with Allocation Preservation:
┌─────────────────────────────────────────────────────────────────────┐
│ Step 1: User triggers recreation                                   │
├─────────────────────────────────────────────────────────────────────┤
│ • Edit YAML: Change destructive field (platform, preset, etc.)     │
│ • Run: nebius-vpngw apply --recreate-gw                            │
│ • Optionally specify external_ips in YAML for explicit IPs        │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 2: Query existing allocations (before VM deletion)            │
├─────────────────────────────────────────────────────────────────────┤
│ • List allocations attached to VM NICs (by VM name)                │
│ • Record allocation IDs and public IP addresses                    │
│ • Store in memory for reassignment after recreation                │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 3: Delete VM only (NOT allocations, NOT subnet)               │
├─────────────────────────────────────────────────────────────────────┤
│ • Call Nebius Compute API: DeleteInstance(instance_id)             │
│ • Allocations automatically detach from NICs (platform behavior)   │
│ • Allocations remain in vpngw-subnet, status: unassigned           │
│ • vpngw-subnet is preserved (NOT deleted)                          │
│ • 30-day countdown starts for each allocation                      │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 4: Create new VM                                              │
├─────────────────────────────────────────────────────────────────────┤
│ • Same deterministic name: <group>-<index>                         │
│ • New platform/preset/disk per YAML changes                        │
│ • New NICs created (attached to vpngw-subnet)                      │
│ • NICs initially have NO public IPs (private IPs only)             │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 5: Allocation matching and assignment                         │
├─────────────────────────────────────────────────────────────────────┤
│ IF external_ips specified in YAML:                                 │
│   • For each external_ip in YAML config:                           │
│     1. Query allocation by ID in vpngw-subnet                      │
│     2. Verify allocation is free (not assigned to other resource)  │
│     3. If free: Attach to corresponding NIC (eth0, eth1, ...)     │
│     4. If not found or busy: ERROR (user must fix YAML)            │
│                                                                     │
│ IF external_ips NOT specified (empty or omitted):                  │
│   • Query unassigned allocations in vpngw-subnet                   │
│   • Match by public IP against previously recorded IPs             │
│   • If matching allocation found:                                  │
│     - Attach to corresponding NIC (preserves IP stability)         │
│   • If no match found:                                             │
│     - Create NEW allocation and attach to NIC                      │
│                                                                     │
│ Priority Order:                                                     │
│   1. Explicit external_ips from YAML (user intent)                 │
│   2. Previously attached allocations (automatic preservation)      │
│   3. Create new allocations (fallback for new deployments)         │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 6: Verify and complete                                        │
├─────────────────────────────────────────────────────────────────────┤
│ • New VM running with same public IPs                              │
│ • Remote peers remain configured correctly (no changes needed)     │
│ • Tunnels re-establish automatically (same endpoints)              │
│ • SSH push new config → agent reload → VPN operational             │
└─────────────────────────────────────────────────────────────────────┘

Allocation Matching Logic (Pseudocode)

```python
def ensure_allocations(vm_instance, yaml_config, previous_allocations):
    """
    Attach public IP allocations to VM NICs with preservation strategy.
    
    Args:
        vm_instance: Newly created VM with NICs
        yaml_config: User YAML with optional external_ips
        previous_allocations: List of allocation IDs from deleted VM (if any)
    """
    num_nics = yaml_config.vm_spec.num_nics
    external_ips = yaml_config.vm_spec.external_ips or []
    
    for nic_index in range(num_nics):
        nic = vm_instance.nics[nic_index]
        
        # Strategy 1: Use explicit external_ip from YAML (highest priority)
        if nic_index < len(external_ips):
            allocation_id = external_ips[nic_index]
            allocation = query_allocation_by_id(allocation_id, subnet="vpngw-subnet")
            if not allocation:
                raise Error(f"Allocation {allocation_id} not found in vpngw-subnet")
            if allocation.status != "unassigned":
                raise Error(f"Allocation {allocation_id} is busy (assigned to {allocation.resource_id})")
            attach_allocation_to_nic(allocation_id, nic.id)
            print(f"[Allocation] Attached explicit allocation {allocation_id} ({allocation.public_ip}) to {nic.name}")
            continue
        
        # Strategy 2: Reuse previous allocation (automatic preservation)
        if nic_index < len(previous_allocations):
            prev_alloc_id = previous_allocations[nic_index]
            allocation = query_allocation_by_id(prev_alloc_id, subnet="vpngw-subnet")
            if allocation and allocation.status == "unassigned":
                # Found previous allocation, still free → reuse it
                attach_allocation_to_nic(allocation.id, nic.id)
                print(f"[Allocation] Preserved allocation {allocation.id} ({allocation.public_ip}) for {nic.name}")
                continue
        
        # Strategy 3: Create new allocation (fallback)
        new_allocation = create_allocation(
            subnet="vpngw-subnet",
            name=f"{vm_instance.name}-{nic.name}-ip"
        )
        attach_allocation_to_nic(new_allocation.id, nic.id)
        print(f"[Allocation] Created new allocation {new_allocation.id} ({new_allocation.public_ip}) for {nic.name}")
```

Benefits of Allocation Preservation

1. **IP Stability**: Public IPs remain constant across VM recreations
2. **Zero Peer Reconfiguration**: Remote VPN peers (GCP, AWS, Azure, on-prem) never need updates
3. **Minimal Downtime**: Only VM recreation time, no waiting for peer config changes
4. **30-Day Safety Net**: Unassigned allocations preserved for 30 days (plenty of time for maintenance)
5. **Explicit Control**: Users can specify exact IPs via external_ips in YAML
6. **Automatic Fallback**: System handles missing allocations gracefully (creates new)

User Scenarios

Scenario 1: Simple VM Recreation (No YAML external_ips)
```yaml
# nebius-vpngw-config.yaml (external_ips NOT specified)
gateway_group:
  name: prod-vpngw
  instance_count: 1
vm_spec:
  platform: cpu-e2  # Changed from cpu-d3 (destructive)
  preset: 4vcpu-16gb
  # external_ips: []  # Omitted or empty
```

Workflow:
1. User runs: `nebius-vpngw apply --recreate-gw`
2. Orchestrator queries existing VM: finds allocation-123 (34.123.45.67) attached to eth0
3. Orchestrator deletes VM → allocation-123 becomes unassigned
4. Orchestrator creates new VM with cpu-e2 platform
5. Orchestrator queries unassigned allocations in vpngw-subnet → finds allocation-123
6. Orchestrator attaches allocation-123 to new VM eth0 → same IP 34.123.45.67
7. Remote peer sees same IP, tunnel re-establishes seamlessly

Scenario 2: Explicit IP Specification
```yaml
# nebius-vpngw-config.yaml (external_ips explicitly specified)
gateway_group:
  name: prod-vpngw
  instance_count: 1
vm_spec:
  platform: cpu-e2
  preset: 4vcpu-16gb
  num_nics: 1
  external_ips:
    - allocation-456  # User wants specific allocation
```

Workflow:
1. User runs: `nebius-vpngw apply --recreate-gw`
2. Orchestrator deletes old VM (detaches allocation-123)
3. Orchestrator creates new VM
4. Orchestrator queries allocation-456 → verifies it exists and is unassigned
5. Orchestrator attaches allocation-456 to new VM eth0
6. VM gets public IP from allocation-456 (might be different IP than before)
7. User must update remote peers if IP changed

Scenario 3: Allocation Not Found (Fallback to New)
```yaml
# Allocation was manually deleted or expired
```

Workflow:
1. User runs: `nebius-vpngw apply --recreate-gw`
2. Orchestrator deletes VM → allocation-123 detaches
3. Orchestrator creates new VM
4. Orchestrator queries for allocation-123 → NOT FOUND (user deleted it)
5. Orchestrator creates NEW allocation: allocation-789 (35.234.56.78)
6. Orchestrator attaches allocation-789 to new VM eth0
7. Public IP changed (34.123.45.67 → 35.234.56.78)
8. User must update remote peers with new IP

Scenario 4: Multi-NIC Gateway (Future)
```yaml
vm_spec:
  num_nics: 2
  external_ips:
    - allocation-111  # Explicit for eth0
    - allocation-222  # Explicit for eth1
```

Workflow:
1. Orchestrator verifies both allocations exist and are free
2. Attaches allocation-111 → eth0, allocation-222 → eth1
3. Both public IPs preserved across recreation

Cleanup Strategy

Manual Cleanup (User Responsibility):
- When decommissioning gateway permanently: `nebius-vpngw destroy` (future command) should:
  1. Delete all VMs
  2. Optionally delete allocations (with --delete-ips flag)
  3. Optionally delete vpngw-subnet (with --delete-subnet flag)
  4. Default: Leave allocations and subnet for manual cleanup
- User can delete allocations and subnet via Nebius Console or API when no longer needed
- Subnet preservation allows quick redeployment without network reconfiguration

Automatic Cleanup (Platform Behavior):
- Nebius AI Cloud deletes unassigned allocations after 30 days
- This is a safety mechanism to prevent orphaned resources
- 30 days is sufficient for maintenance windows and troubleshooting

Best Practices:
1. **Always specify external_ips in YAML** for production gateways (explicit > implicit)
2. **Document allocation IDs** in runbooks/documentation for disaster recovery
3. **Monitor unassigned allocations** to avoid unexpected 30-day deletions
4. **Reassign within 30 days** if performing long maintenance windows
5. **Use tags/labels** on allocations to track ownership and purpose

Implementation Notes

VMManager.ensure_group() Logic:
```python
def ensure_group(self, recreate: bool = False):
    for i in range(instance_count):
        instance_name = f"{group_name}-{i}"
        
        # Track previous allocations if recreating
        previous_allocations = []
        if recreate:
            existing_vm = get_vm_by_name(instance_name)
            if existing_vm:
                previous_allocations = get_attached_allocations(existing_vm)
                delete_vm(existing_vm)  # Does NOT delete allocations
        
        # Create or reuse VM
        vm = create_or_get_vm(instance_name, vm_spec)
        
        # Attach allocations with preservation strategy
        ensure_allocations(vm, yaml_config, previous_allocations)
```

Key API Calls:
- `vpc.AllocationService.List(subnet_id, status=unassigned)` - Find free allocations
- `vpc.AllocationService.Get(allocation_id)` - Query specific allocation
- `compute.NetworkInterfaceService.AttachPublicIP(nic_id, allocation_id)` - Attach to NIC
- Platform automatically detaches allocations when VM/NIC deleted (no explicit call needed)

Error Handling:
- Allocation not found → Create new allocation (fallback)
- Allocation busy (assigned) → ERROR with clear message (user must fix YAML)
- Multiple unassigned allocations found → Use first match by creation time (oldest first)
- No subnet match → ERROR (allocation must be in vpngw-subnet)

Summary
- **Default behavior**: Preserve allocations on VM recreation (IP stability)
- **Explicit control**: Users can specify exact allocations via external_ips
- **Automatic fallback**: Create new allocations if previous ones unavailable
- **30-day grace**: Nebius preserves unassigned allocations for maintenance windows
- **Production-ready**: VPN gateways maintain stable public IPs for remote peers
- **User cleanup**: Manual deletion of allocations when truly decommissioning

═══════════════════════════════════════════════════════════════════════════════

17) Nebius SDK API Compliance & Corrections
═══════════════════════════════════════════════════════════════════════════════

Overview
This section documents the review and correction of subnet creation code to ensure
compliance with the official Nebius Python SDK API specifications.

**Date**: 2024-01-XX (code review and SDK compliance validation)
**Scope**: `_ensure_vpngw_subnet` method in `vm_manager.py`
**SDK Version**: nebius 0.3.x from PyPI
**Documentation Reference**: https://github.com/nebius/pysdk and https://nebius.github.io/pysdk/

Issue Identified
----------------
The original subnet creation code used Python dictionaries to construct the
`ipv4_private_pools` parameter:

```python
# INCORRECT: Using dict instead of SDK message objects
ipv4_private_pools = {
    "pools": [{"cidrs": [{"cidr": cidr_to_use}]}],
    "use_network_pools": False,
}
```

This approach violated the Nebius SDK schema, which requires proper message objects:

- `SubnetSpec.ipv4_private_pools` expects type `IPv4PrivateSubnetPools` (message object)
- `IPv4PrivateSubnetPools.pools` expects `Iterable[SubnetPool]` (list of message objects)
- `SubnetPool.cidrs` expects `Iterable[SubnetCidr]` (list of message objects)

SDK Schema (Official)
---------------------
Based on Nebius pysdk documentation:

**SubnetSpec** (nebius.api.nebius.vpc.v1.SubnetSpec):
- `network_id: str` — ID of the network this subnet belongs to (required)
- `ipv4_private_pools: IPv4PrivateSubnetPools` — Private IPv4 pools for subnet
- `ipv4_public_pools: IPv4PublicSubnetPools` — Public IPv4 pools (optional)
- `route_table_id: str` — Route table association (optional)

**IPv4PrivateSubnetPools** (nebius.api.nebius.vpc.v1.IPv4PrivateSubnetPools):
- `pools: Iterable[SubnetPool]` — List of private CIDR blocks
- `use_network_pools: bool` — If true, inherit from network; must be false if pools specified

**SubnetPool** (nebius.api.nebius.vpc.v1.SubnetPool):
- `cidrs: Iterable[SubnetCidr]` — List of CIDR blocks in this pool

**SubnetCidr** (nebius.api.nebius.vpc.v1.SubnetCidr):
- `cidr: str` — CIDR block string (e.g., "10.0.1.0/27")
- `max_mask_length: int` — Optional maximum mask length
- `state: AddressBlockState` — Optional state field

Corrected Implementation
-------------------------
Updated code now uses proper SDK message objects:

```python
from nebius.api.nebius.vpc.v1 import (
    IPv4PrivateSubnetPools,
    SubnetPool,
    SubnetCidr,
    # ... other imports
)

# CORRECT: Using SDK message objects
if cidr_to_use:
    ipv4_private_pools = IPv4PrivateSubnetPools(
        pools=[
            SubnetPool(
                cidrs=[
                    SubnetCidr(cidr=cidr_to_use)
                ]
            )
        ],
        use_network_pools=False,
    )
else:
    # Fallback: inherit network pools
    ipv4_private_pools = IPv4PrivateSubnetPools(use_network_pools=True)

# Then pass to CreateSubnetRequest
req = CreateSubnetRequest(
    metadata=ResourceMetadata(name="vpngw-subnet", parent_id=project_id),
    spec=SubnetSpec(network_id=net_id, ipv4_private_pools=ipv4_private_pools),
)
```

Validation Results
------------------
✅ Imports updated to include: IPv4PrivateSubnetPools, SubnetPool, SubnetCidr
✅ Code uses proper message object constructors
✅ Nested structure matches SDK schema exactly
✅ No Python syntax errors (validated via get_errors tool)
✅ Backward compatible with existing logic (CIDR calculation unchanged)

Benefits of Correction
----------------------
1. **Type Safety**: SDK enforces proper types at runtime
2. **API Compatibility**: Ensures future SDK updates work correctly
3. **Error Detection**: Invalid fields rejected by SDK before API call
4. **Documentation Alignment**: Code matches official SDK examples
5. **Production Ready**: Reduces risk of runtime API errors

Related Code Sections
---------------------
- **File**: `src/nebius_vpngw/deploy/vm_manager.py`
- **Method**: `_ensure_vpngw_subnet` (lines 851-1043)
- **Imports**: Lines 863-876
- **Construction**: Lines 995-1012
- **Usage**: Line 1015 (CreateSubnetRequest)

Testing Recommendations
-----------------------
1. Verify subnet creation in empty network (CIDR derivation)
2. Test subnet reuse on subsequent runs (idempotency)
3. Validate with explicit network_id in YAML
4. Confirm fallback to default-network works
5. Test error handling when network has no IP pools

Conclusion
----------
The subnet creation code now fully complies with the Nebius Python SDK API
specifications. All message objects are constructed using proper SDK classes,
ensuring type safety, API compatibility, and production reliability.

End of Section 17.

18) Agent Deployment and Package Build Workflow

Overview
--------
The Nebius VPN Gateway uses a two-tier architecture for code deployment:
- **Orchestrator CLI** (`nebius-vpngw`): Runs on operator's laptop/CI (installed in local virtualenv)
- **Agent daemon** (`nebius-vpngw-agent`): Runs on gateway VMs (deployed via wheel package)

The orchestrator and agent are distributed from the same Python package but installed
in different locations. Understanding the build and deployment workflow is critical
for ensuring that code changes are properly deployed to VMs.

Package Distribution Model
---------------------------
1. **Local Development Environment (Operator)**
   - Install: `pip install -e .` or `poetry install`
   - Location: Operator's virtualenv (e.g., `~/venvs/vpngw/`)
   - Usage: Run `nebius-vpngw` CLI to manage VMs
   - Updates: Immediate (editable install)

2. **Gateway VMs (Production)**
   - Install: `pip install <wheel>` via SSH push
   - Location: VM's system Python (`/usr/bin/nebius-vpngw-agent`)
   - Usage: Agent daemon reads config, renders strongSwan/FRR
   - Updates: Only when wheel is rebuilt and redeployed

Key Insight: The operator's local virtualenv and the VM's Python environment are
completely separate. Changes to agent code require rebuilding and redeploying the
wheel package to VMs.

Deployment Workflow (Step-by-Step)
-----------------------------------
┌─────────────────────────────────────────────────────────────────────┐
│ Step 1: Modify Agent Code                                          │
├─────────────────────────────────────────────────────────────────────┤
│ • Edit files in src/nebius_vpngw/agent/                           │
│ • Example: Fix bug in main.py signal handling                      │
│ • Changes are immediately available in local virtualenv (editable) │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 2: Build Wheel Package                                        │
├─────────────────────────────────────────────────────────────────────┤
│ Clean old artifacts:                                                │
│   rm -rf src/dist/ src/build/ src/nebius_vpngw.egg-info/          │
│                                                                     │
│ Build fresh wheel (choose one method):                             │
│   poetry build                    # Recommended with Poetry        │
│   python -m build --wheel         # Alternative with build module  │
│   pip wheel --no-deps -w dist .   # Alternative with pip           │
│                                                                     │
│ Result: dist/nebius_vpngw-0.0.0-py3-none-any.whl                  │
│                                                                     │
│ Note: Wheel is NOT installed into local virtualenv—it's only       │
│       packaged for deployment to VMs.                              │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 3: Deploy to VMs                                              │
├─────────────────────────────────────────────────────────────────────┤
│ Set environment variables:                                          │
│   export GCP_TUNNEL_PSK="your-secret"                              │
│   export NETWORK_ID="vpcnetwork-xxxxx"                             │
│                                                                     │
│ Run orchestrator:                                                   │
│   nebius-vpngw --local-config-file ./nebius-vpngw-config.yaml     │
│                                                                     │
│ Orchestrator actions:                                               │
│   1. Read wheel from dist/ directory                               │
│   2. SSH to each gateway VM                                        │
│   3. Upload wheel to VM (/tmp/nebius_vpngw-*.whl)                 │
│   4. Install wheel: pip install --force-reinstall <wheel>          │
│   5. Install agent wrapper: /usr/bin/nebius-vpngw-agent           │
│   6. Push config: /etc/nebius-vpngw/config-resolved.yaml          │
│   7. Reload agent: systemctl reload nebius-vpngw-agent            │
└─────────────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────────────┐
│ Step 4: Agent Receives Updated Code                                │
├─────────────────────────────────────────────────────────────────────┤
│ On VM:                                                              │
│   • Old agent process (PID 4097) receives SIGHUP                   │
│   • Agent reloads config from /etc/nebius-vpngw/config-resolved.yaml│
│   • New code (e.g., while True loop) is now active                 │
│   • Agent renders ipsec.conf, bgpd.conf, ipsec.secrets             │
│   • Agent persists state to last-applied.json                      │
│   • Agent stays running (new behavior from fixed code)             │
└─────────────────────────────────────────────────────────────────────┘

SSH Push Implementation (src/nebius_vpngw/deploy/ssh_push.py)
--------------------------------------------------------------
The SSHPush class handles wheel deployment automatically:

```python
def _build_wheel(self) -> Optional[Path]:
    """Build the nebius-vpngw wheel package if not already built."""
    # Check for existing wheel in dist/
    dist_dir = project_root / "dist"
    if dist_dir.exists():
        wheels = list(dist_dir.glob("nebius_vpngw-*.whl"))
        if wheels:
            print(f"[SSHPush] Using existing wheel: {wheels[0].name}")
            return wheels[0]
    
    # Build fresh wheel using python -m build
    print("[SSHPush] Building nebius-vpngw wheel package...")
    result = subprocess.run(
        ["python3", "-m", "build", "--wheel"],
        cwd=project_root,
        capture_output=True,
    )
    # ... (checks for success and returns wheel path)

def push_and_reload(self, vm_ip: str, config: dict) -> None:
    """Deploy package and config to VM, reload agent."""
    # 1. Build/find wheel
    wheel_path = self._build_wheel()
    
    # 2. Upload wheel via SFTP
    sftp.put(str(wheel_path), f"/tmp/{wheel_path.name}")
    
    # 3. Install on VM
    ssh.exec_command(f"pip3 install --force-reinstall /tmp/{wheel_path.name}")
    
    # 4. Install agent wrapper script
    ssh.exec_command("ln -sf $(which nebius-vpngw-agent) /usr/bin/")
    
    # 5. Push config
    sftp.put("/tmp/config.yaml", "/etc/nebius-vpngw/config-resolved.yaml")
    
    # 6. Reload agent
    ssh.exec_command("systemctl reload nebius-vpngw-agent")
```

Common Pitfalls and Solutions
------------------------------
❌ **Pitfall 1: Editing agent code but not rebuilding wheel**
   - Symptom: Changes not reflected on VMs after deployment
   - Cause: Orchestrator uploads old wheel from dist/
   - Solution: Always rebuild wheel after code changes

✅ **Solution:**
   ```bash
   rm -rf src/dist/ src/build/ src/nebius_vpngw.egg-info/
   poetry build
   nebius-vpngw --local-config-file ./nebius-vpngw-config.yaml
   ```

❌ **Pitfall 2: Confusion between local venv and VM Python**
   - Symptom: "It works locally but not on VMs"
   - Cause: Local editable install vs VM wheel install
   - Solution: Remember they're separate environments

✅ **Solution:**
   - Local: `pip install -e .` → changes take effect immediately
   - VMs: Must rebuild wheel and redeploy

❌ **Pitfall 3: Using cached wheel after bug fixes**
   - Symptom: Bug persists on VMs after local fix
   - Cause: dist/ contains old wheel from previous build
   - Solution: Clean dist/ before rebuilding

✅ **Solution:**
   ```bash
   rm -rf src/dist/  # Force fresh build
   poetry build
   ```

Development Best Practices
---------------------------
1. **Test Locally First**
   - Run `poetry install` for editable install
   - Test changes with `nebius-vpngw` CLI locally
   - Verify config rendering works correctly

2. **Rebuild Wheel for VM Deployment**
   - Clean old build artifacts: `rm -rf src/dist/`
   - Build fresh wheel: `poetry build`
   - Verify wheel exists: `ls -lh dist/`

3. **Deploy and Verify**
   - Deploy: `nebius-vpngw --local-config-file ...`
   - SSH to VM: `ssh ubuntu@<vm-ip>`
   - Check agent status: `sudo systemctl status nebius-vpngw-agent`
   - View agent logs: `sudo journalctl -u nebius-vpngw-agent -n 50`
   - Verify rendered configs:
     - `sudo cat /etc/ipsec.conf`
     - `sudo cat /etc/frr/bgpd.conf`

4. **Iterate Quickly**
   - For rapid iteration, use SSH push without full VM recreation
   - Only rebuild wheel when agent code changes
   - Config-only changes don't require wheel rebuild

Integration with CI/CD
-----------------------
Recommended CI workflow:

```yaml
# .github/workflows/deploy.yml
- name: Build wheel package
  run: |
    poetry install
    rm -rf src/dist/ src/build/
    poetry build
    
- name: Deploy to staging VMs
  run: |
    export GCP_TUNNEL_PSK="${{ secrets.GCP_TUNNEL_PSK }}"
    poetry run nebius-vpngw --local-config-file ./staging.yaml
    
- name: Verify agent health
  run: |
    ssh ubuntu@staging-vm "sudo systemctl is-active nebius-vpngw-agent"
```

Wheel Versioning Strategy
--------------------------
Current: `nebius_vpngw-0.0.0-py3-none-any.whl` (development)

Future versioning options:
1. **Semantic Versioning**: Update version in pyproject.toml for releases
2. **Git-based Versioning**: Use git describe or commit SHA in version
3. **Timestamp Versioning**: Append build timestamp for uniqueness

Example versioning update:
```toml
# pyproject.toml
[tool.poetry]
name = "nebius-vpngw"
version = "0.1.0"  # Increment for releases
```

Summary
-------
**Key Takeaways:**
1. Agent code changes require wheel rebuild before deployment
2. Local virtualenv and VM Python are separate environments
3. Always clean dist/ before rebuilding to avoid stale wheels
4. Use `poetry build` or `python -m build --wheel` to rebuild
5. The orchestrator automatically uploads and installs the wheel on VMs
6. Verify deployment success by checking agent status on VMs

**Quick Reference Commands:**
```bash
# Rebuild wheel after code changes
rm -rf src/dist/ src/build/ src/nebius_vpngw.egg-info/
poetry build

# Deploy to VMs
export GCP_TUNNEL_PSK="your-secret"
nebius-vpngw --local-config-file ./nebius-vpngw-config.yaml

# Verify on VM
ssh ubuntu@<vm-ip> 'sudo systemctl status nebius-vpngw-agent'
ssh ubuntu@<vm-ip> 'sudo journalctl -u nebius-vpngw-agent -n 20'
```

End of Section 18.